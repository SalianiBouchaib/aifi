import streamlit as st

# Configuration de la page - DOIT √™tre la premi√®re commande Streamlit
st.set_page_config(
    page_title="AIFI - Advanced Financial Intelligence Suite", 
    layout="wide",
    initial_sidebar_state="expanded",
    page_icon="üåç"
)

# Imports enrichis
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import json
import os
import warnings
from datetime import datetime, timedelta
import base64
from io import BytesIO
import time

# Imports g√©ospatiaux
import folium
from folium import plugins
from folium.plugins import HeatMap, MarkerCluster
import pydeck as pdk
from geopy.geocoders import Nominatim
from geopy.distance import geodesic

# Handle optional dependencies with try/except
try:
    from sklearn.ensemble import RandomForestRegressor
    from sklearn.linear_model import LinearRegression
    from sklearn.preprocessing import StandardScaler
    from sklearn.cluster import KMeans
    ML_AVAILABLE = True
except ImportError:
    ML_AVAILABLE = False
    class MLFallback:
        def __init__(self, *args, **kwargs):
            pass
        def fit(self, X, y):
            pass
        def predict(self, X):
            return [0] * len(X)
        def fit_transform(self, X):
            return X
        def transform(self, X):
            return X
    RandomForestRegressor = MLFallback
    LinearRegression = MLFallback
    StandardScaler = MLFallback
    KMeans = MLFallback

warnings.filterwarnings('ignore')

# ========== ENHANCED CSV DATA MANAGER ==========
class EnhancedCSVDataManager:
    """Gestionnaire de donn√©es CSV enrichi avec fonctionnalit√©s g√©ospatiales"""
    
    @staticmethod
    def get_csv_financial_data():
        """R√©cup√®re les donn√©es financi√®res avec informations g√©ographiques"""
        if not st.session_state.imported_metrics:
            return None
        
        metrics = st.session_state.imported_metrics
        financial_data = {}
        
        # Donn√©es financi√®res de base
        if 'revenue' in metrics:
            financial_data['revenue'] = metrics['revenue']['average'] * 12
            financial_data['monthly_revenue'] = metrics['revenue']['average']
            financial_data['revenue_data'] = metrics['revenue']['data']
            financial_data['revenue_growth'] = metrics['revenue'].get('growth_rate', 0)
            financial_data['revenue_volatility'] = metrics['revenue'].get('volatility', 0)
        
        if 'costs' in metrics:
            financial_data['total_costs'] = metrics['costs']['average'] * 12
            financial_data['monthly_costs'] = metrics['costs']['average']
            financial_data['costs_data'] = metrics['costs']['data']
            financial_data['costs_growth'] = metrics['costs'].get('growth_rate', 0)
        
        if 'profit' in metrics:
            financial_data['net_profit'] = metrics['profit']['average'] * 12
            financial_data['monthly_profit'] = metrics['profit']['average']
            financial_data['profit_data'] = metrics['profit']['data']
            financial_data['profit_margin'] = metrics['profit'].get('margin_average', 0)
        
        # Donn√©es g√©ographiques enrichies
        if 'geographic_data' in metrics:
            geo_data = metrics['geographic_data']
            financial_data.update({
                'total_locations': geo_data.get('total_locations', 0),
                'geographic_spread': geo_data.get('geographic_spread', 0),
                'best_performing_location': geo_data.get('best_location', {}),
                'geographic_clusters': geo_data.get('clusters', []),
                'regional_performance': geo_data.get('regional_metrics', {})
            })
        
        # M√©triques business enrichies
        if 'business_metrics' in metrics:
            biz_data = metrics['business_metrics']
            financial_data.update({
                'customer_count': biz_data.get('customer_count', 0),
                'customer_acquisition_cost': biz_data.get('cac', 0),
                'lifetime_value': biz_data.get('ltv', 0),
                'market_share': biz_data.get('market_share', 0),
                'competition_level': biz_data.get('competition_level', 0),
                'brand_strength': biz_data.get('brand_strength', 0)
            })
        
        # Calculs d√©riv√©s avec nouvelles m√©triques
        if 'revenue' in financial_data and 'total_costs' in financial_data:
            financial_data['gross_profit'] = financial_data['revenue'] - financial_data['total_costs']
            financial_data['operating_profit'] = financial_data['gross_profit'] * 0.8
            financial_data['net_margin'] = financial_data['net_profit'] / financial_data['revenue'] if financial_data['revenue'] > 0 else 0
        
        # Estimations am√©lior√©es du bilan
        if 'revenue' in financial_data:
            revenue = financial_data['revenue']
            financial_data.update({
                'current_assets': revenue * 0.3,
                'current_liabilities': revenue * 0.15,
                'total_assets': revenue * 0.8,
                'total_debt': revenue * 0.2,
                'equity': revenue * 0.4,
                'cash': revenue * 0.1,
                'inventory': revenue * 0.08,
                'interest_expense': revenue * 0.02,
                'cash_flow': financial_data.get('monthly_profit', revenue * 0.02),
                'current_ratio': (revenue * 0.3) / (revenue * 0.15),
                'debt_to_equity': (revenue * 0.2) / (revenue * 0.4),
                'interest_coverage': (financial_data['gross_profit'] * 0.8) / (revenue * 0.02)
            })
        
        return financial_data
    
    @staticmethod
    def has_csv_data():
        return bool(st.session_state.imported_metrics)
    
    @staticmethod
    def get_csv_insights():
        if 'csv_data' in st.session_state and 'insights' in st.session_state.csv_data:
            return st.session_state.csv_data['insights']
        return None
    
    @staticmethod
    def get_csv_visualizations():
        if 'csv_data' in st.session_state and 'figures' in st.session_state.csv_data:
            return st.session_state.csv_data['figures']
        return None
    
    @staticmethod
    def get_geographic_data():
        """R√©cup√®re les donn√©es g√©ographiques sp√©cifiques"""
        if 'csv_data' in st.session_state and 'geographic_analysis' in st.session_state.csv_data:
            return st.session_state.csv_data['geographic_analysis']
        return None

# ========== ENHANCED CSV PROCESSOR WITH GEOSPATIAL ==========
class EnhancedGeoCSVProcessor:
    def __init__(self):
        self.column_mappings = {
            # Colonnes financi√®res de base
            'revenue': ['revenue', 'sales', 'income', 'turnover', 'receipts', 'monthly_recurring_revenue', 'mrr', 'arr'],
            'costs': ['costs', 'expenses', 'expenditure', 'outgoings', 'total_costs', 'variable_costs', 'fixed_costs', 'cost_of_goods_sold', 'cogs'],
            'date': ['date', 'month', 'period', 'time', 'year', 'quarter'],
            'profit': ['profit', 'earnings', 'net_income', 'net income', 'pnl', 'p&l', 'operating_profit', 'gross_profit'],
            
            # Colonnes g√©ographiques
            'latitude': ['latitude', 'lat', 'y', 'coord_y', 'geo_lat'],
            'longitude': ['longitude', 'lng', 'lon', 'x', 'coord_x', 'geo_lng'],
            'location_id': ['location_id', 'store_id', 'branch_id', 'office_id', 'site_id'],
            'location_name': ['location_name', 'store', 'branch', 'office', 'site', 'location'],
            'city': ['city', 'ville', 'town', 'municipality'],
            'region': ['region', 'state', 'province', 'area', 'zone'],
            'country': ['country', 'pays', 'nation'],
            'postal_code': ['postal_code', 'zip_code', 'zip', 'code_postal'],
            
            # M√©triques business enrichies
            'customer_count': ['customer_count', 'customers', 'active_users', 'monthly_active_users', 'mau', 'clients'],
            'customer_acquisition_cost': ['customer_acquisition_cost', 'cac', 'acquisition_cost'],
            'lifetime_value': ['lifetime_value', 'ltv', 'customer_lifetime_value', 'clv'],
            'churn_rate': ['churn_rate', 'churn', 'attrition_rate'],
            'retention_rate': ['retention_rate', 'retention'],
            'market_share': ['market_share', 'market_position', 'share'],
            'competition_level': ['competition_level', 'competitive_intensity', 'competition'],
            'brand_strength': ['brand_strength', 'brand_awareness', 'brand_score'],
            'demographics_score': ['demographics_score', 'demographic_index', 'population_score'],
            
            # M√©triques op√©rationnelles
            'units_sold': ['units_sold', 'quantity', 'volume', 'transactions', 'sales_volume'],
            'average_price': ['average_price', 'price_per_unit', 'average_transaction_value', 'atv'],
            'inventory_level': ['inventory_level', 'stock_level', 'inventory'],
            'employee_count': ['employee_count', 'employees', 'staff', 'workforce'],
            'store_size': ['store_size', 'square_feet', 'surface', 'area'],
            
            # M√©triques financi√®res avanc√©es
            'cash_flow': ['cash_flow', 'cash flow', 'cashflow', 'cash', 'flow'],
            'assets': ['assets', 'total_assets', 'current_assets', 'fixed_assets'],
            'liabilities': ['liabilities', 'total_liabilities', 'current_liabilities', 'debt'],
            'equity': ['equity', 'shareholders_equity', 'owners_equity'],
            'accounts_receivable': ['accounts_receivable', 'receivables', 'ar', 'debtors'],
            'accounts_payable': ['accounts_payable', 'payables', 'ap', 'creditors'],
            
            # M√©triques marketing
            'marketing_spend': ['marketing_spend', 'marketing_budget', 'advertising_cost'],
            'lead_generation': ['lead_generation', 'leads', 'prospects'],
            'conversion_rate': ['conversion_rate', 'conversion', 'close_rate'],
            'website_traffic': ['website_traffic', 'web_visitors', 'online_traffic'],
            'social_media_engagement': ['social_engagement', 'social_score', 'engagement_rate']
        }
        
        self.required_columns = ['date']
        self.detected_mappings = {}
        self.analysis_results = {}
        self.geolocator = Nominatim(user_agent="aifi_enhanced_v2")
    
    def detect_columns(self, df):
        """D√©tection avanc√©e des colonnes avec priorisation g√©ographique"""
        detected = {}
        
        for target, keywords in self.column_mappings.items():
            for col in df.columns:
                col_lower = col.lower().replace('_', ' ').replace('-', ' ')
                for keyword in keywords:
                    if keyword in col_lower or col_lower in keyword:
                        if target not in detected:
                            detected[target] = []
                        detected[target].append(col)
                        break
        
        # Prioriser les colonnes exactes
        for target in detected:
            detected[target] = list(set(detected[target]))
            if len(detected[target]) > 1:
                exact_matches = [col for col in detected[target] 
                               if col.lower().replace('_', ' ') in self.column_mappings[target]]
                if exact_matches:
                    detected[target] = exact_matches[:1]
                else:
                    detected[target] = detected[target][:1]
        
        # Aplatir en une seule colonne par target
        for target in detected:
            if detected[target]:
                detected[target] = detected[target][0]
        
        self.detected_mappings = detected
        return detected
    
    def geocode_locations(self, df, location_column):
        """G√©ocodage automatique des emplacements"""
        geocoded_data = []
        
        for location in df[location_column].unique():
            if pd.isna(location) or location == '':
                continue
                
            try:
                location_info = self.geolocator.geocode(location, timeout=10)
                if location_info:
                    geocoded_data.append({
                        'location': location,
                        'latitude': location_info.latitude,
                        'longitude': location_info.longitude,
                        'full_address': location_info.address
                    })
                else:
                    # Fallback: essayer avec le pays ajout√©
                    location_with_country = f"{location}, Morocco"
                    location_info = self.geolocator.geocode(location_with_country, timeout=10)
                    if location_info:
                        geocoded_data.append({
                            'location': location,
                            'latitude': location_info.latitude,
                            'longitude': location_info.longitude,
                            'full_address': location_info.address
                        })
                
                # Pause pour √©viter le rate limiting
                time.sleep(0.1)
                
            except Exception as e:
                st.warning(f"Erreur g√©ocodage pour {location}: {e}")
        
        return pd.DataFrame(geocoded_data)
    
    def calculate_enhanced_metrics(self, df, mappings):
        """Calcul des m√©triques enrichies incluant g√©ospatiales"""
        metrics = {}
        
        # M√©triques financi√®res de base
        if 'revenue' in mappings and mappings['revenue'] in df.columns:
            revenue_col = mappings['revenue']
            revenue_data = self.clean_numeric_column(df[revenue_col]).dropna()
            
            metrics['revenue'] = {
                'total': revenue_data.sum(),
                'average': revenue_data.mean(),
                'median': revenue_data.median(),
                'std': revenue_data.std(),
                'min': revenue_data.min(),
                'max': revenue_data.max(),
                'trend': 'increasing' if len(revenue_data) > 1 and revenue_data.iloc[-1] > revenue_data.iloc[0] else 'decreasing',
                'growth_rate': ((revenue_data.iloc[-1] / revenue_data.iloc[0]) - 1) * 100 if len(revenue_data) > 1 and revenue_data.iloc[0] != 0 else 0,
                'volatility': revenue_data.std() / revenue_data.mean() if revenue_data.mean() != 0 else 0,
                'data': revenue_data.tolist()
            }
        
        if 'costs' in mappings and mappings['costs'] in df.columns:
            costs_col = mappings['costs']
            costs_data = self.clean_numeric_column(df[costs_col]).dropna()
            
            metrics['costs'] = {
                'total': costs_data.sum(),
                'average': costs_data.mean(),
                'median': costs_data.median(),
                'std': costs_data.std(),
                'min': costs_data.min(),
                'max': costs_data.max(),
                'trend': 'increasing' if len(costs_data) > 1 and costs_data.iloc[-1] > costs_data.iloc[0] else 'decreasing',
                'growth_rate': ((costs_data.iloc[-1] / costs_data.iloc[0]) - 1) * 100 if len(costs_data) > 1 and costs_data.iloc[0] != 0 else 0,
                'volatility': costs_data.std() / costs_data.mean() if costs_data.mean() != 0 else 0,
                'data': costs_data.tolist()
            }
        
        # Calcul du profit si revenus et co√ªts disponibles
        if 'revenue' in metrics and 'costs' in metrics:
            revenue_data = np.array(metrics['revenue']['data'])
            costs_data = np.array(metrics['costs']['data'])
            
            min_length = min(len(revenue_data), len(costs_data))
            revenue_data = revenue_data[:min_length]
            costs_data = costs_data[:min_length]
            
            profit_data = revenue_data - costs_data
            
            metrics['profit'] = {
                'total': profit_data.sum(),
                'average': profit_data.mean(),
                'median': np.median(profit_data),
                'std': np.std(profit_data),
                'min': profit_data.min(),
                'max': profit_data.max(),
                'trend': 'increasing' if len(profit_data) > 1 and profit_data[-1] > profit_data[0] else 'decreasing',
                'margin_average': (profit_data.mean() / revenue_data.mean() * 100) if revenue_data.mean() != 0 else 0,
                'data': profit_data.tolist()
            }
        
        # M√©triques g√©ographiques
        if self.has_geographic_data(mappings, df):
            geo_metrics = self.calculate_geographic_metrics(df, mappings)
            metrics['geographic_data'] = geo_metrics
        
        # M√©triques business enrichies
        business_metrics = self.calculate_business_metrics(df, mappings)
        if business_metrics:
            metrics['business_metrics'] = business_metrics
        
        self.analysis_results = metrics
        return metrics
    
    def has_geographic_data(self, mappings, df):
        """V√©rifie la pr√©sence de donn√©es g√©ographiques"""
        return ('latitude' in mappings and 'longitude' in mappings and 
                mappings['latitude'] in df.columns and mappings['longitude'] in df.columns)
    
    def calculate_geographic_metrics(self, df, mappings):
        """Calcul des m√©triques g√©ographiques"""
        geo_metrics = {}
        
        lat_col = mappings['latitude']
        lon_col = mappings['longitude']
        
        # Donn√©es g√©ographiques de base
        valid_coords = df[[lat_col, lon_col]].dropna()
        
        if len(valid_coords) > 0:
            geo_metrics['total_locations'] = len(valid_coords)
            geo_metrics['center_latitude'] = valid_coords[lat_col].mean()
            geo_metrics['center_longitude'] = valid_coords[lon_col].mean()
            geo_metrics['geographic_spread'] = self.calculate_geographic_spread(valid_coords, lat_col, lon_col)
            
            # Analyse par localisation si revenus disponibles
            if 'revenue' in mappings and mappings['revenue'] in df.columns:
                location_performance = self.analyze_location_performance(df, mappings)
                geo_metrics.update(location_performance)
            
            # Clustering g√©ographique
            if len(valid_coords) >= 3:
                clusters = self.perform_geographic_clustering(df, mappings)
                geo_metrics['clusters'] = clusters
        
        return geo_metrics
    
    def calculate_geographic_spread(self, coords_df, lat_col, lon_col):
        """Calcule l'√©tendue g√©ographique maximale"""
        if len(coords_df) < 2:
            return 0
        
        max_distance = 0
        for i in range(len(coords_df)):
            for j in range(i+1, len(coords_df)):
                coord1 = (coords_df.iloc[i][lat_col], coords_df.iloc[i][lon_col])
                coord2 = (coords_df.iloc[j][lat_col], coords_df.iloc[j][lon_col])
                try:
                    distance = geodesic(coord1, coord2).kilometers
                    max_distance = max(max_distance, distance)
                except:
                    continue
        
        return max_distance
    
    def analyze_location_performance(self, df, mappings):
        """Analyse des performances par localisation"""
        performance_data = {}
        
        lat_col = mappings['latitude']
        lon_col = mappings['longitude']
        revenue_col = mappings['revenue']
        
        # Agr√©gation par localisation
        if 'location_name' in mappings:
            location_col = mappings['location_name']
            location_stats = df.groupby(location_col).agg({
                lat_col: 'first',
                lon_col: 'first',
                revenue_col: ['sum', 'mean', 'count']
            }).round(2)
            
            # Meilleure localisation
            best_location = location_stats[revenue_col]['sum'].idxmax()
            best_revenue = location_stats.loc[best_location, (revenue_col, 'sum')]
            
            performance_data['best_location'] = {
                'name': best_location,
                'revenue': best_revenue,
                'latitude': location_stats.loc[best_location, lat_col],
                'longitude': location_stats.loc[best_location, lon_col]
            }
            
            # Statistiques r√©gionales
            performance_data['location_stats'] = location_stats.to_dict()
        
        return performance_data
    
    def perform_geographic_clustering(self, df, mappings):
        """Clustering g√©ographique K-means"""
        lat_col = mappings['latitude']
        lon_col = mappings['longitude']
        revenue_col = mappings.get('revenue')
        
        coords = df[[lat_col, lon_col]].dropna()
        
        if len(coords) < 3:
            return []
        
        # K-means clustering
        n_clusters = min(3, len(coords))
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        clusters = kmeans.fit_predict(coords)
        
        cluster_analysis = []
        for cluster_id in range(n_clusters):
            cluster_mask = clusters == cluster_id
            cluster_coords = coords[cluster_mask]
            cluster_data = df.loc[cluster_coords.index]
            
            cluster_info = {
                'cluster_id': cluster_id,
                'location_count': len(cluster_data),
                'center_lat': kmeans.cluster_centers_[cluster_id][0],
                'center_lon': kmeans.cluster_centers_[cluster_id][1],
                'geographic_bounds': {
                    'min_lat': cluster_coords[lat_col].min(),
                    'max_lat': cluster_coords[lat_col].max(),
                    'min_lon': cluster_coords[lon_col].min(),
                    'max_lon': cluster_coords[lon_col].max()
                }
            }
            
            # Performance du cluster si revenus disponibles
            if revenue_col and revenue_col in cluster_data.columns:
                cluster_revenue = self.clean_numeric_column(cluster_data[revenue_col]).dropna()
                if len(cluster_revenue) > 0:
                    cluster_info['avg_performance'] = cluster_revenue.mean()
                    cluster_info['total_performance'] = cluster_revenue.sum()
                    cluster_info['performance_level'] = 'High' if cluster_revenue.mean() > df[revenue_col].mean() else 'Low'
            
            cluster_analysis.append(cluster_info)
        
        return cluster_analysis
    
    def calculate_business_metrics(self, df, mappings):
        """Calcul des m√©triques business enrichies"""
        business_metrics = {}
        
        # M√©triques clients
        if 'customer_count' in mappings and mappings['customer_count'] in df.columns:
            customer_data = self.clean_numeric_column(df[mappings['customer_count']]).dropna()
            business_metrics['customer_count'] = customer_data.mean()
            business_metrics['customer_growth'] = ((customer_data.iloc[-1] / customer_data.iloc[0]) - 1) * 100 if len(customer_data) > 1 else 0
        
        # CAC et LTV
        if 'customer_acquisition_cost' in mappings and mappings['customer_acquisition_cost'] in df.columns:
            cac_data = self.clean_numeric_column(df[mappings['customer_acquisition_cost']]).dropna()
            business_metrics['cac'] = cac_data.mean()
        
        if 'lifetime_value' in mappings and mappings['lifetime_value'] in df.columns:
            ltv_data = self.clean_numeric_column(df[mappings['lifetime_value']]).dropna()
            business_metrics['ltv'] = ltv_data.mean()
        
        # Ratio LTV/CAC
        if 'ltv' in business_metrics and 'cac' in business_metrics and business_metrics['cac'] > 0:
            business_metrics['ltv_cac_ratio'] = business_metrics['ltv'] / business_metrics['cac']
        
        # M√©triques de march√©
        if 'market_share' in mappings and mappings['market_share'] in df.columns:
            market_data = self.clean_numeric_column(df[mappings['market_share']]).dropna()
            business_metrics['market_share'] = market_data.mean()
        
        if 'competition_level' in mappings and mappings['competition_level'] in df.columns:
            comp_data = self.clean_numeric_column(df[mappings['competition_level']]).dropna()
            business_metrics['competition_level'] = comp_data.mean()
        
        if 'brand_strength' in mappings and mappings['brand_strength'] in df.columns:
            brand_data = self.clean_numeric_column(df[mappings['brand_strength']]).dropna()
            business_metrics['brand_strength'] = brand_data.mean()
        
        return business_metrics
    
    def clean_numeric_column(self, series):
        """Nettoyage des colonnes num√©riques"""
        if pd.api.types.is_numeric_dtype(series):
            return series
        
        cleaned = series.astype(str)
        cleaned = cleaned.str.replace(r'[$‚Ç¨¬£¬•‚Çπ‚ÇΩ%]', '', regex=True)
        cleaned = cleaned.str.replace(',', '')
        cleaned = cleaned.str.replace(' ', '')
        cleaned = cleaned.str.replace(r'[^\d.-]', '', regex=True)
        
        cleaned = pd.to_numeric(cleaned, errors='coerce')
        return cleaned
    
    def standardize_date_column(self, series):
        """Standardisation des colonnes de date"""
        try:
            return pd.to_datetime(series, infer_datetime_format=True)
        except:
            formats = ['%Y-%m-%d', '%Y-%m', '%m/%d/%Y', '%d/%m/%Y', '%Y', '%m-%Y']
            for fmt in formats:
                try:
                    return pd.to_datetime(series, format=fmt)
                except:
                    continue
            return series
    
    def generate_enhanced_insights(self, metrics):
        """G√©n√©ration d'insights enrichis avec g√©ospatial"""
        insights = []
        recommendations = []
        alerts = []
        
        # Insights financiers
        if 'revenue' in metrics:
            rev_metrics = metrics['revenue']
            
            if rev_metrics['trend'] == 'increasing':
                insights.append(f"üìà **Croissance Revenue Positive**: {rev_metrics['growth_rate']:.1f}% sur la p√©riode")
            else:
                alerts.append(f"üìâ **D√©clin Revenue**: {abs(rev_metrics['growth_rate']):.1f}% de baisse d√©tect√©e")
                recommendations.append("Analyser les causes du d√©clin et d√©velopper des strat√©gies de relance")
            
            if rev_metrics['volatility'] > 0.3:
                alerts.append(f"üìä **Forte Volatilit√© Revenue**: {rev_metrics['volatility']:.1%} coefficient de variation")
                recommendations.append("Consid√©rer des strat√©gies pour stabiliser les flux de revenus")
            elif rev_metrics['volatility'] < 0.1:
                insights.append("‚úÖ **Revenue Stable**: Faible volatilit√© indique une performance pr√©visible")
        
        # Insights g√©ographiques
        if 'geographic_data' in metrics:
            geo_data = metrics['geographic_data']
            
            if geo_data.get('total_locations', 0) > 1:
                insights.append(f"üåç **Pr√©sence Multi-localisations**: {geo_data['total_locations']} emplacements analys√©s")
                
                spread = geo_data.get('geographic_spread', 0)
                if spread > 100:
                    insights.append(f"üìç **Large Couverture G√©ographique**: {spread:.0f} km d'√©tendue")
                    recommendations.append("Optimiser la logistique et la coordination entre sites distants")
                
                if 'best_location' in geo_data:
                    best_loc = geo_data['best_location']
                    insights.append(f"üèÜ **Meilleur Site**: {best_loc['name']} avec {best_loc['revenue']:,.0f} DHS")
                
                if 'clusters' in geo_data and len(geo_data['clusters']) > 1:
                    high_perf_clusters = [c for c in geo_data['clusters'] if c.get('performance_level') == 'High']
                    if high_perf_clusters:
                        insights.append(f"üéØ **Clusters Haute Performance**: {len(high_perf_clusters)} zones identifi√©es")
                        recommendations.append("R√©pliquer les strat√©gies des zones performantes sur les autres sites")
        
        # Insights business
        if 'business_metrics' in metrics:
            biz_data = metrics['business_metrics']
            
            if 'ltv_cac_ratio' in biz_data:
                ratio = biz_data['ltv_cac_ratio']
                if ratio > 3:
                    insights.append(f"üí∞ **Excellent LTV/CAC**: Ratio de {ratio:.1f} (optimal > 3)")
                elif ratio > 1:
                    insights.append(f"üìä **LTV/CAC Correct**: Ratio de {ratio:.1f} (am√©lioration possible)")
                else:
                    alerts.append(f"üî¥ **LTV/CAC Critique**: Ratio de {ratio:.1f} (< 1)")
                    recommendations.append("Urgent: R√©duire les co√ªts d'acquisition ou augmenter la valeur client")
            
            if 'market_share' in biz_data:
                share = biz_data['market_share'] * 100
                if share > 20:
                    insights.append(f"üéØ **Part de March√© Forte**: {share:.1f}% du march√©")
                elif share > 10:
                    insights.append(f"üìà **Part de March√© Correcte**: {share:.1f}% du march√©")
                else:
                    recommendations.append(f"üìä **Opportunit√© Croissance**: {share:.1f}% part de march√© actuelle")
        
        return {
            'insights': insights,
            'recommendations': recommendations,
            'alerts': alerts
        }
    
    def create_enhanced_visualizations(self, df, mappings, metrics):
        """Cr√©ation de visualisations enrichies"""
        figures = {}
        
        # Graphique financier temporel standard
        if 'date' in mappings and mappings['date'] in df.columns:
            time_col = self.standardize_date_column(df[mappings['date']])
            x_axis = time_col
            x_title = "Date"
        else:
            x_axis = range(len(df))
            x_title = "P√©riode"
        
        # Graphique performance financi√®re
        if 'revenue' in mappings or 'costs' in mappings:
            fig = go.Figure()
            
            if 'revenue' in mappings and mappings['revenue'] in df.columns:
                revenue_data = self.clean_numeric_column(df[mappings['revenue']])
                fig.add_trace(go.Scatter(
                    x=x_axis,
                    y=revenue_data,
                    mode='lines+markers',
                    name='Revenue',
                    line=dict(color='green', width=3),
                    marker=dict(size=8)
                ))
            
            if 'costs' in mappings and mappings['costs'] in df.columns:
                costs_data = self.clean_numeric_column(df[mappings['costs']])
                fig.add_trace(go.Scatter(
                    x=x_axis,
                    y=costs_data,
                    mode='lines+markers',
                    name='Co√ªts',
                    line=dict(color='red', width=3),
                    marker=dict(size=8)
                ))
            
            if 'revenue' in mappings and 'costs' in mappings:
                profit_data = revenue_data - costs_data
                fig.add_trace(go.Scatter(
                    x=x_axis,
                    y=profit_data,
                    mode='lines+markers',
                    name='Profit',
                    line=dict(color='blue', width=3),
                    marker=dict(size=8)
                ))
            
            fig.update_layout(
                title="Performance Financi√®re dans le Temps",
                xaxis_title=x_title,
                yaxis_title="Montant (DHS)",
                hovermode='x unified',
                height=500
            )
            
            figures['financial_trend'] = fig
        
        # Graphiques g√©ographiques
        if self.has_geographic_data(mappings, df):
            geo_figs = self.create_geographic_visualizations(df, mappings, metrics)
            figures.update(geo_figs)
        
        # Graphiques business m√©triques
        if 'business_metrics' in metrics:
            business_figs = self.create_business_visualizations(df, mappings, metrics)
            figures.update(business_figs)
        
        return figures
    
    def create_geographic_visualizations(self, df, mappings, metrics):
        """Cr√©ation des visualisations g√©ographiques"""
        geo_figures = {}
        
        lat_col = mappings['latitude']
        lon_col = mappings['longitude']
        revenue_col = mappings.get('revenue')
        
        # Carte scatter g√©ographique
        if revenue_col:
            revenue_data = self.clean_numeric_column(df[revenue_col])
            
            fig = go.Figure()
            
            # Taille des marqueurs bas√©e sur les revenus
            marker_sizes = (revenue_data / revenue_data.max() * 50 + 10).fillna(10)
            
            fig.add_trace(go.Scattermapbox(
                lat=df[lat_col],
                lon=df[lon_col],
                mode='markers',
                marker=dict(
                    size=marker_sizes,
                    color=revenue_data,
                    colorscale='Viridis',
                    showscale=True,
                    colorbar=dict(title="Revenue (DHS)")
                ),
                text=[f"Revenue: {rev:,.0f} DHS" for rev in revenue_data],
                hovertemplate='<b>%{text}</b><br>Lat: %{lat}<br>Lon: %{lon}<extra></extra>',
                name='Performance par Localisation'
            ))
            
            fig.update_layout(
                mapbox=dict(
                    style='open-street-map',
                    center=dict(
                        lat=df[lat_col].mean(),
                        lon=df[lon_col].mean()
                    ),
                    zoom=6
                ),
                height=600,
                title="Carte des Performances par Localisation"
            )
            
            geo_figures['performance_map'] = fig
        
        # Heatmap des clusters si disponible
        if 'geographic_data' in metrics and 'clusters' in metrics['geographic_data']:
            clusters = metrics['geographic_data']['clusters']
            
            if len(clusters) > 1:
                cluster_fig = go.Figure()
                
                colors = ['red', 'blue', 'green', 'orange', 'purple']
                
                for i, cluster in enumerate(clusters):
                    cluster_color = colors[i % len(colors)]
                    
                    cluster_fig.add_trace(go.Scattermapbox(
                        lat=[cluster['center_lat']],
                        lon=[cluster['center_lon']],
                        mode='markers',
                        marker=dict(size=25, color=cluster_color),
                        name=f"Cluster {i+1}",
                        text=f"Cluster {i+1}<br>Emplacements: {cluster['location_count']}<br>Performance: {cluster.get('performance_level', 'N/A')}",
                        hovertemplate='%{text}<extra></extra>'
                    ))
                
                cluster_fig.update_layout(
                    mapbox=dict(
                        style='open-street-map',
                        center=dict(
                            lat=df[lat_col].mean(),
                            lon=df[lon_col].mean()
                        ),
                        zoom=6
                    ),
                    height=500,
                    title="Clusters G√©ographiques de Performance"
                )
                
                geo_figures['clusters_map'] = cluster_fig
        
        return geo_figures
    
    def create_business_visualizations(self, df, mappings, metrics):
        """Cr√©ation des visualisations m√©triques business"""
        business_figures = {}
        
        biz_metrics = metrics['business_metrics']
        
        # Graphique en radar des m√©triques business
        if len(biz_metrics) >= 3:
            metrics_names = []
            metrics_values = []
            
            metric_mapping = {
                'customer_count': 'Nombre Clients',
                'cac': 'Co√ªt Acquisition',
                'ltv': 'Valeur Vie Client',
                'market_share': 'Part de March√©',
                'competition_level': 'Niveau Concurrence',
                'brand_strength': 'Force Marque'
            }
            
            for key, value in biz_metrics.items():
                if key in metric_mapping and isinstance(value, (int, float)):
                    metrics_names.append(metric_mapping[key])
                    # Normaliser les valeurs pour le radar (0-100)
                    if key == 'market_share':
                        normalized_value = value * 100
                    elif key == 'competition_level':
                        normalized_value = (1 - value) * 100  # Inverser car moins de concurrence = mieux
                    else:
                        # Normalisation simple
                        normalized_value = min(100, max(0, value / 1000 * 100))
                    metrics_values.append(normalized_value)
            
            if len(metrics_names) >= 3:
                radar_fig = go.Figure()
                
                radar_fig.add_trace(go.Scatterpolar(
                    r=metrics_values,
                    theta=metrics_names,
                    fill='toself',
                    name='M√©triques Business',
                    line_color='rgba(0,100,200,0.8)',
                    fillcolor='rgba(0,100,200,0.2)'
                ))
                
                radar_fig.update_layout(
                    polar=dict(
                        radialaxis=dict(
                            visible=True,
                            range=[0, 100]
                        )),
                    showlegend=False,
                    title="Radar des M√©triques Business",
                    height=500
                )
                
                business_figures['business_radar'] = radar_fig
        
        return business_figures
    
    def process_csv(self, df):
        """Traitement principal du CSV enrichi"""
        mappings = self.detect_columns(df)
        issues, suggestions = self.validate_data(df, mappings)
        metrics = self.calculate_enhanced_metrics(df, mappings)
        insights_data = self.generate_enhanced_insights(metrics)
        figures = self.create_enhanced_visualizations(df, mappings, metrics)
        
        # Analyse g√©ographique approfondie si donn√©es disponibles
        geographic_analysis = None
        if self.has_geographic_data(mappings, df):
            geographic_analysis = self.perform_comprehensive_geographic_analysis(df, mappings, metrics)
        
        return {
            'mappings': mappings,
            'metrics': metrics,
            'insights': insights_data,
            'figures': figures,
            'issues': issues,
            'suggestions': suggestions,
            'processed_df': df,
            'geographic_analysis': geographic_analysis
        }
    
    def perform_comprehensive_geographic_analysis(self, df, mappings, metrics):
        """Analyse g√©ographique compl√®te"""
        geo_analysis = {}
        
        lat_col = mappings['latitude']
        lon_col = mappings['longitude']
        revenue_col = mappings.get('revenue')
        
        # Statistiques g√©ographiques de base
        geo_analysis['basic_stats'] = {
            'total_locations': len(df[[lat_col, lon_col]].dropna()),
            'center_point': {
                'latitude': df[lat_col].mean(),
                'longitude': df[lon_col].mean()
            },
            'geographic_bounds': {
                'north': df[lat_col].max(),
                'south': df[lat_col].min(),
                'east': df[lon_col].max(),
                'west': df[lon_col].min()
            }
        }
        
        # Analyse de densit√© si suffisamment de points
        if len(df) >= 5:
            geo_analysis['density_analysis'] = self.calculate_density_metrics(df, mappings)
        
        # Corr√©lations g√©ographiques avec performance
        if revenue_col:
            geo_analysis['geo_performance_correlation'] = self.analyze_geo_performance_correlation(df, mappings)
        
        return geo_analysis
    
    def calculate_density_metrics(self, df, mappings):
        """Calcul des m√©triques de densit√© g√©ographique"""
        lat_col = mappings['latitude']
        lon_col = mappings['longitude']
        
        # Calculer la densit√© de points par r√©gion
        # Diviser l'espace en grille et compter les points
        lat_range = df[lat_col].max() - df[lat_col].min()
        lon_range = df[lon_col].max() - df[lon_col].min()
        
        density_metrics = {
            'geographic_concentration': 1 / (lat_range * lon_range) if lat_range > 0 and lon_range > 0 else 0,
            'average_distance_between_points': self.calculate_average_distance(df, lat_col, lon_col)
        }
        
        return density_metrics
    
    def calculate_average_distance(self, df, lat_col, lon_col):
        """Calcule la distance moyenne entre tous les points"""
        coords = df[[lat_col, lon_col]].dropna()
        
        if len(coords) < 2:
            return 0
        
        total_distance = 0
        count = 0
        
        for i in range(len(coords)):
            for j in range(i+1, len(coords)):
                coord1 = (coords.iloc[i][lat_col], coords.iloc[i][lon_col])
                coord2 = (coords.iloc[j][lat_col], coords.iloc[j][lon_col])
                try:
                    distance = geodesic(coord1, coord2).kilometers
                    total_distance += distance
                    count += 1
                except:
                    continue
        
        return total_distance / count if count > 0 else 0
    
    def analyze_geo_performance_correlation(self, df, mappings):
        """Analyse de corr√©lation entre g√©ographie et performance"""
        lat_col = mappings['latitude']
        lon_col = mappings['longitude']
        revenue_col = mappings['revenue']
        
        revenue_data = self.clean_numeric_column(df[revenue_col])
        
        # Corr√©lations avec latitude/longitude
        lat_corr = df[lat_col].corr(revenue_data)
        lon_corr = df[lon_col].corr(revenue_data)
        
        # Analyse par quadrants g√©ographiques
        center_lat = df[lat_col].mean()
        center_lon = df[lon_col].mean()
        
        quadrants = {
            'north_east': df[(df[lat_col] >= center_lat) & (df[lon_col] >= center_lon)],
            'north_west': df[(df[lat_col] >= center_lat) & (df[lon_col] < center_lon)],
            'south_east': df[(df[lat_col] < center_lat) & (df[lon_col] >= center_lon)],
            'south_west': df[(df[lat_col] < center_lat) & (df[lon_col] < center_lon)]
        }
        
        quadrant_performance = {}
        for quad_name, quad_data in quadrants.items():
            if len(quad_data) > 0:
                quad_revenue = self.clean_numeric_column(quad_data[revenue_col])
                quadrant_performance[quad_name] = {
                    'count': len(quad_data),
                    'avg_revenue': quad_revenue.mean(),
                    'total_revenue': quad_revenue.sum()
                }
        
        return {
            'latitude_correlation': lat_corr,
            'longitude_correlation': lon_corr,
            'quadrant_analysis': quadrant_performance
        }
    
    def validate_data(self, df, mappings):
        """Validation des donn√©es enrichie"""
        issues = []
        suggestions = []
        
        # Validation g√©ographique
        if 'latitude' in mappings and 'longitude' in mappings:
            lat_col = mappings['latitude']
            lon_col = mappings['longitude']
            
            # V√©rifier les coordonn√©es valides
            valid_lat = df[lat_col].between(-90, 90).sum()
            valid_lon = df[lon_col].between(-180, 180).sum()
            
            if valid_lat < len(df):
                issues.append(f"Coordonn√©es latitude invalides d√©tect√©es: {len(df) - valid_lat} lignes")
                suggestions.append("V√©rifier les valeurs de latitude (doivent √™tre entre -90 et 90)")
            
            if valid_lon < len(df):
                issues.append(f"Coordonn√©es longitude invalides d√©tect√©es: {len(df) - valid_lon} lignes")
                suggestions.append("V√©rifier les valeurs de longitude (doivent √™tre entre -180 et 180)")
        
        # Validation business m√©triques
        if 'customer_acquisition_cost' in mappings and 'lifetime_value' in mappings:
            cac_col = mappings['customer_acquisition_cost']
            ltv_col = mappings['lifetime_value']
            
            cac_data = self.clean_numeric_column(df[cac_col])
            ltv_data = self.clean_numeric_column(df[ltv_col])
            
            # V√©rifier le ratio LTV/CAC
            ltv_cac_ratio = (ltv_data / cac_data).mean()
            if ltv_cac_ratio < 3:
                issues.append(f"Ratio LTV/CAC faible: {ltv_cac_ratio:.1f} (recommand√© > 3)")
                suggestions.append("Am√©liorer la r√©tention client ou r√©duire les co√ªts d'acquisition")
        
        # Validations standard
        if 'date' not in mappings:
            suggestions.append("Ajouter une colonne date pour l'analyse temporelle")
        
        if 'revenue' not in mappings:
            issues.append("Aucune colonne revenue d√©tect√©e - critique pour l'analyse financi√®re")
            suggestions.append("S'assurer d'avoir une colonne revenue/sales")
        
        return issues, suggestions

# ========== ENHANCED TEMPLATE GENERATOR ==========
class EnhancedCSVTemplateGenerator:
    def __init__(self):
        self.templates = {
            'complete_geo_financial': {
                'name': 'Template Financier G√©ographique Complet',
                'description': 'Template complet avec donn√©es financi√®res, g√©ographiques et business',
                'columns': {
                    'Date': 'YYYY-MM-DD (ex: 2025-06-28)',
                    'Location_ID': 'Identifiant unique de localisation (ex: LOC001)',
                    'Location_Name': 'Nom de l\'emplacement (ex: Casablanca_Centre)',
                    'Latitude': 'Latitude en degr√©s d√©cimaux (ex: 33.5731)',
                    'Longitude': 'Longitude en degr√©s d√©cimaux (ex: -7.5898)',
                    'City': 'Ville (ex: Casablanca)',
                    'Region': 'R√©gion (ex: Grand Casablanca)',
                    'Country': 'Pays (ex: Morocco)',
                    'Postal_Code': 'Code postal (ex: 20000)',
                    'Revenue': 'Revenus en monnaie locale (ex: 15000)',
                    'Costs': 'Co√ªts totaux (ex: 12000)',
                    'Profit': 'Profit net (ex: 3000)',
                    'Cash_Flow': 'Flux de tr√©sorerie (ex: 2500)',
                    'Customer_Count': 'Nombre de clients actifs (ex: 150)',
                    'Customer_Acquisition_Cost': 'Co√ªt d\'acquisition client (ex: 200)',
                    'Lifetime_Value': 'Valeur vie client (ex: 1500)',
                    'Churn_Rate': 'Taux d\'attrition (0.05 = 5%)',
                    'Market_Share': 'Part de march√© (0.25 = 25%)',
                    'Competition_Level': 'Niveau concurrence (0.7 = √©lev√©)',
                    'Brand_Strength': 'Force de marque (0-1)',
                    'Units_Sold': 'Unit√©s vendues (ex: 300)',
                    'Average_Price': 'Prix moyen (ex: 50)',
                    'Employee_Count': 'Nombre d\'employ√©s (ex: 10)',
                    'Store_Size': 'Taille magasin en m¬≤ (ex: 200)',
                    'Marketing_Spend': 'D√©penses marketing (ex: 2000)',
                    'Website_Traffic': 'Trafic web mensuel (ex: 5000)',
                    'Demographics_Score': 'Score d√©mographique zone (0-1)'
                },
                'sample_data': self.generate_complete_sample_data()
            },
            'retail_geo': {
                'name': 'Template Retail G√©olocalis√©',
                'description': 'Sp√©cialis√© pour commerce de d√©tail avec g√©olocalisation',
                'columns': {
                    'Date': 'Date de l\'analyse',
                    'Store_ID': 'Identifiant magasin',
                    'Store_Name': 'Nom du magasin',
                    'Latitude': 'Latitude',
                    'Longitude': 'Longitude',
                    'City': 'Ville',
                    'Revenue': 'Chiffre d\'affaires',
                    'Foot_Traffic': 'Trafic pi√©ton',
                    'Conversion_Rate': 'Taux de conversion',
                    'Average_Basket': 'Panier moyen',
                    'Inventory_Level': 'Niveau stock',
                    'Competition_Proximity': 'Proximit√© concurrence (km)'
                },
                'sample_data': self.generate_retail_sample_data()
            },
            'saas_geo': {
                'name': 'Template SaaS G√©ographique',
                'description': 'Pour entreprises SaaS avec analyse g√©ographique clients',
                'columns': {
                    'Date': 'Date',
                    'Region_ID': 'ID r√©gion',
                    'Region_Name': 'Nom r√©gion',
                    'Latitude': 'Latitude centre r√©gion',
                    'Longitude': 'Longitude centre r√©gion',
                    'MRR': 'Revenue r√©current mensuel',
                    'Active_Users': 'Utilisateurs actifs',
                    'Churn_Rate': 'Taux de churn',
                    'CAC': 'Co√ªt acquisition client',
                    'LTV': 'Valeur vie client',
                    'Support_Tickets': 'Tickets support',
                    'Local_Language': 'Langue locale'
                },
                'sample_data': self.generate_saas_sample_data()
            }
        }
    
    def generate_complete_sample_data(self):
        """G√©n√®re des donn√©es d'exemple compl√®tes"""
        dates = ['2025-06-01', '2025-06-15', '2025-06-28'] * 4
        
        # Donn√©es pour diff√©rentes villes marocaines
        locations = [
            {'id': 'CAS001', 'name': 'Casablanca_Centre', 'lat': 33.5731, 'lon': -7.5898, 'city': 'Casablanca', 'region': 'Grand Casablanca', 'postal': '20000'},
            {'id': 'RAB001', 'name': 'Rabat_Agdal', 'lat': 34.0209, 'lon': -6.8416, 'city': 'Rabat', 'region': 'Rabat-Sal√©-K√©nitra', 'postal': '10000'},
            {'id': 'MAR001', 'name': 'Marrakech_Gueliz', 'lat': 31.6295, 'lon': -7.9811, 'city': 'Marrakech', 'region': 'Marrakech-Safi', 'postal': '40000'},
            {'id': 'FES001', 'name': 'F√®s_Ville_Nouvelle', 'lat': 34.0181, 'lon': -5.0078, 'city': 'F√®s', 'region': 'F√®s-Mekn√®s', 'postal': '30000'}
        ]
        
        sample_data = []
        for i, date in enumerate(dates):
            loc = locations[i % len(locations)]
            
            # Variations r√©alistes dans les donn√©es
            base_revenue = 15000 if loc['id'] == 'CAS001' else 12000 if loc['id'] == 'RAB001' else 8500 if loc['id'] == 'MAR001' else 9200
            revenue_variation = np.random.normal(1, 0.1)
            revenue = base_revenue * revenue_variation
            
            costs = revenue * 0.75  # 75% du revenue en co√ªts
            profit = revenue - costs
            
            sample_data.append([
                date,
                loc['id'],
                loc['name'],
                loc['lat'],
                loc['lon'],
                loc['city'],
                loc['region'],
                'Morocco',
                loc['postal'],
                round(revenue, 0),
                round(costs, 0),
                round(profit, 0),
                round(profit * 0.8, 0),  # Cash flow
                round(revenue / 100),  # Customer count
                round(200 + np.random.normal(0, 50), 0),  # CAC
                round(1500 + np.random.normal(0, 300), 0),  # LTV
                round(np.random.uniform(0.03, 0.08), 3),  # Churn rate
                round(np.random.uniform(0.1, 0.3), 2),  # Market share
                round(np.random.uniform(0.5, 0.9), 2),  # Competition level
                round(np.random.uniform(0.6, 0.9), 2),  # Brand strength
                round(revenue / 50),  # Units sold
                50,  # Average price
                round(revenue / 1500),  # Employee count
                200 + (i % 4) * 50,  # Store size
                round(revenue * 0.15, 0),  # Marketing spend
                round(revenue * 0.3, 0),  # Website traffic
                round(np.random.uniform(0.6, 0.9), 2)  # Demographics score
            ])
        
        return sample_data
    
    def generate_retail_sample_data(self):
        """Donn√©es d'exemple retail"""
        return [
            ['2025-06-28', 'ST001', 'Mega Mall Casa', 33.5731, -7.5898, 'Casablanca', 25000, 1200, 0.12, 85, 150, 0.5],
            ['2025-06-28', 'ST002', 'Marina Shopping Rabat', 34.0209, -6.8416, 'Rabat', 18000, 950, 0.10, 75, 120, 0.8],
            ['2025-06-28', 'ST003', 'Menara Mall Marrakech', 31.6295, -7.9811, 'Marrakech', 15000, 800, 0.14, 90, 100, 1.2]
        ]
    
    def generate_saas_sample_data(self):
        """Donn√©es d'exemple SaaS"""
        return [
            ['2025-06-28', 'REG001', 'North_Africa', 33.0, -7.0, 25000, 500, 0.05, 150, 1800, 45, 'French'],
            ['2025-06-28', 'REG002', 'West_Africa', 14.0, -14.0, 18000, 350, 0.07, 200, 1500, 62, 'French'],
            ['2025-06-28', 'REG003', 'Middle_East', 24.0, 45.0, 22000, 420, 0.04, 180, 2000, 38, 'Arabic']
        ]
    
    def generate_template_csv(self, template_type):
        """G√©n√®re un CSV template"""
        template = self.templates.get(template_type)
        if not template:
            return None
        
        columns = list(template['columns'].keys())
        df = pd.DataFrame(template['sample_data'], columns=columns)
        
        csv_buffer = BytesIO()
        df.to_csv(csv_buffer, index=False, encoding='utf-8')
        csv_buffer.seek(0)
        
        return csv_buffer.getvalue()

# ========== GEOGRAPHIC ANALYZER ==========
class GeoFinancialAnalyzer:
    def __init__(self):
        self.geolocator = Nominatim(user_agent="aifi_enhanced_v2")
        self.maps_cache = {}
    
    def create_performance_heatmap(self, df, lat_col, lon_col, value_col, location_col=None):
        """Cr√©ation de heatmap Folium avanc√©e"""
        # Centre sur le Maroc par d√©faut ou donn√©es
        center_lat = df[lat_col].mean() if not df.empty else 31.7917
        center_lon = df[lon_col].mean() if not df.empty else -7.0926
        
        # Carte Folium avec style personnalis√©
        m = folium.Map(
            location=[center_lat, center_lon],
            zoom_start=6,
            tiles='OpenStreetMap'
        )
        
        # Ajouter des tiles alternatives
        folium.TileLayer('cartodbpositron', name='CartoDB Positron').add_to(m)
        folium.TileLayer('cartodbdark_matter', name='CartoDB Dark').add_to(m)
        
        # Donn√©es pour heatmap
        heat_data = []
        marker_data = []
        
        for idx, row in df.iterrows():
            if pd.notna(row[lat_col]) and pd.notna(row[lon_col]) and pd.notna(row[value_col]):
                lat, lon, value = row[lat_col], row[lon_col], row[value_col]
                
                # Intensit√© pour heatmap (0.1                 # Intensit√© pour heatmap (0.1 √† 1.0)
                max_value = df[value_col].max()
                intensity = max(0.1, min(1.0, value / max_value)) if max_value > 0 else 0.1
                heat_data.append([lat, lon, intensity])
                
                # Donn√©es pour marqueurs individuels
                location_name = row[location_col] if location_col and location_col in df.columns else f"Point {idx}"
                popup_text = f"""
                <b>{location_name}</b><br>
                üí∞ Valeur: {value:,.0f} DHS<br>
                üìç Coordonn√©es: {lat:.4f}, {lon:.4f}<br>
                üìä Performance: {(value/max_value*100):.1f}%
                """
                
                # Couleur du marqueur bas√©e sur la performance
                if value >= max_value * 0.8:
                    marker_color = 'green'
                    icon = 'star'
                elif value >= max_value * 0.6:
                    marker_color = 'blue'
                    icon = 'info-sign'
                elif value >= max_value * 0.4:
                    marker_color = 'orange'
                    icon = 'warning-sign'
                else:
                    marker_color = 'red'
                    icon = 'remove'
                
                marker_data.append({
                    'lat': lat, 'lon': lon, 'popup': popup_text,
                    'color': marker_color, 'icon': icon, 'value': value
                })
        
        # Ajouter la heatmap
        if heat_data:
            HeatMap(
                heat_data,
                min_opacity=0.3,
                max_zoom=18,
                radius=20,
                blur=15,
                gradient={0.2: 'blue', 0.4: 'cyan', 0.6: 'lime', 0.8: 'yellow', 1.0: 'red'}
            ).add_to(m)
        
        # Ajouter les marqueurs avec clustering
        if marker_data:
            marker_cluster = MarkerCluster(name='Performance Points').add_to(m)
            
            for marker in marker_data:
                folium.Marker(
                    location=[marker['lat'], marker['lon']],
                    popup=folium.Popup(marker['popup'], max_width=300),
                    tooltip=f"Valeur: {marker['value']:,.0f} DHS",
                    icon=folium.Icon(color=marker['color'], icon=marker['icon'])
                ).add_to(marker_cluster)
        
        # Ajouter contr√¥les de couches
        folium.LayerControl().add_to(m)
        
        # Ajouter plugin fullscreen
        plugins.Fullscreen().add_to(m)
        
        return m
    
    def create_3d_deck_map(self, df, lat_col, lon_col, value_col):
        """Carte 3D avec PyDeck am√©lior√©e"""
        # Normaliser les valeurs pour l'√©l√©vation
        max_val = df[value_col].max()
        min_val = df[value_col].min()
        
        # Pr√©parer les donn√©es
        deck_data = df.copy()
        deck_data['elevation'] = ((deck_data[value_col] - min_val) / (max_val - min_val) * 2000).fillna(0)
        deck_data['color'] = deck_data[value_col].apply(self.value_to_color)
        
        # Layer colonnes 3D
        column_layer = pdk.Layer(
            'ColumnLayer',
            data=deck_data,
            get_position=[lon_col, lat_col],
            get_elevation='elevation',
            elevation_scale=4,
            radius=1000,
            get_fill_color='color',
            pickable=True,
            extruded=True,
        )
        
        # Layer heatmap
        heatmap_layer = pdk.Layer(
            'HeatmapLayer',
            data=deck_data,
            get_position=[lon_col, lat_col],
            get_weight=value_col,
            radius_pixels=100,
        )
        
        # Vue initiale
        view_state = pdk.ViewState(
            longitude=df[lon_col].mean(),
            latitude=df[lat_col].mean(),
            zoom=6,
            min_zoom=5,
            max_zoom=15,
            pitch=45,
            bearing=0
        )
        
        # Tooltip personnalis√©
        tooltip = {
            "html": f"<b>Performance</b><br/>{value_col}: {{{value_col}}}<br/>Coordonn√©es: {{{lat_col}}}, {{{lon_col}}}",
            "style": {"backgroundColor": "steelblue", "color": "white"}
        }
        
        # Deck avec layers multiples
        deck = pdk.Deck(
            layers=[column_layer, heatmap_layer],
            initial_view_state=view_state,
            tooltip=tooltip,
            map_style='mapbox://styles/mapbox/light-v9'
        )
        
        return deck
    
    def value_to_color(self, value):
        """Convertit une valeur en couleur RGB"""
        # Normalisation 0-255 pour RGB
        normalized = min(255, max(0, value / 20000 * 255))
        
        if normalized < 85:
            return [255, int(normalized * 3), 0, 200]  # Rouge vers orange
        elif normalized < 170:
            return [int(255 - (normalized - 85) * 3), 255, 0, 200]  # Orange vers vert
        else:
            return [0, 255, int((normalized - 170) * 3), 200]  # Vert vers cyan
    
    def create_plotly_choropleth_map(self, df, lat_col, lon_col, value_col, location_col=None):
        """Carte choropl√®he avec Plotly"""
        
        # Taille des marqueurs
        marker_sizes = (df[value_col] / df[value_col].max() * 80 + 15).fillna(15)
        
        # Couleurs bas√©es sur les performances
        colors = df[value_col].fillna(0)
        
        # Texte de survol personnalis√©
        hover_text = []
        for idx, row in df.iterrows():
            location_name = row[location_col] if location_col and location_col in df.columns else f"Localisation {idx}"
            hover_info = f"""
            <b>{location_name}</b><br>
            üí∞ {value_col}: {row[value_col]:,.0f} DHS<br>
            üìç Latitude: {row[lat_col]:.4f}<br>
            üìç Longitude: {row[lon_col]:.4f}<br>
            üìä Rang: {df[value_col].rank(method='dense', ascending=False)[idx]:.0f}/{len(df)}
            """
            hover_text.append(hover_info)
        
        # Cr√©ation de la figure
        fig = go.Figure()
        
        # Couche principale avec marqueurs
        fig.add_trace(go.Scattermapbox(
            lat=df[lat_col],
            lon=df[lon_col],
            mode='markers',
            marker=dict(
                size=marker_sizes,
                color=colors,
                colorscale='Viridis',
                showscale=True,
                colorbar=dict(
                    title=f"{value_col}<br>(DHS)",
                    titleside="right",
                    tickmode="linear",
                    tick0=colors.min(),
                    dtick=(colors.max() - colors.min()) / 5
                ),
                opacity=0.8,
                line=dict(width=2, color='white')
            ),
            text=hover_text,
            hovertemplate='%{text}<extra></extra>',
            name='Performance par Zone'
        ))
        
        # Ajouter des cercles de performance
        for idx, row in df.iterrows():
            if pd.notna(row[value_col]):
                performance_ratio = row[value_col] / df[value_col].max()
                radius = performance_ratio * 0.1  # Rayon en degr√©s
                
                fig.add_trace(go.Scattermapbox(
                    lat=[row[lat_col]],
                    lon=[row[lon_col]],
                    mode='markers',
                    marker=dict(
                        size=radius * 1000,
                        color='rgba(255,0,0,0.2)',
                        opacity=0.3
                    ),
                    showlegend=False,
                    hoverinfo='skip'
                ))
        
        # Configuration de la carte
        fig.update_layout(
            mapbox=dict(
                style='open-street-map',
                center=dict(
                    lat=df[lat_col].mean(),
                    lon=df[lon_col].mean()
                ),
                zoom=6
            ),
            height=700,
            title={
                'text': f"Carte Interactive des Performances - {value_col}",
                'x': 0.5,
                'xanchor': 'center',
                'font': {'size': 18}
            },
            showlegend=True,
            legend=dict(
                yanchor="top",
                y=0.99,
                xanchor="left",
                x=0.01
            )
        )
        
        return fig
    
    def analyze_geographic_clusters(self, df, lat_col, lon_col, value_col, n_clusters=None):
        """Analyse avanc√©e des clusters g√©ographiques"""
        coords = df[[lat_col, lon_col]].dropna()
        
        if len(coords) < 3:
            return []
        
        # D√©terminer le nombre optimal de clusters
        if n_clusters is None:
            n_clusters = min(5, max(2, len(coords) // 3))
        
        # K-means clustering
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(coords)
        
        # Analyse d√©taill√©e par cluster
        cluster_analysis = []
        
        for cluster_id in range(n_clusters):
            cluster_mask = cluster_labels == cluster_id
            cluster_coords = coords[cluster_mask]
            cluster_data = df.loc[cluster_coords.index]
            
            # M√©triques g√©ographiques du cluster
            cluster_center = kmeans.cluster_centers_[cluster_id]
            
            # Calcul du rayon du cluster
            distances_to_center = []
            for idx in cluster_coords.index:
                point = (cluster_coords.loc[idx, lat_col], cluster_coords.loc[idx, lon_col])
                center = (cluster_center[0], cluster_center[1])
                try:
                    dist = geodesic(center, point).kilometers
                    distances_to_center.append(dist)
                except:
                    distances_to_center.append(0)
            
            cluster_radius = np.mean(distances_to_center) if distances_to_center else 0
            
            # M√©triques de performance
            cluster_values = self.clean_numeric_column(cluster_data[value_col]).dropna()
            
            cluster_info = {
                'cluster_id': cluster_id,
                'location_count': len(cluster_data),
                'center_lat': cluster_center[0],
                'center_lon': cluster_center[1],
                'radius_km': cluster_radius,
                'geographic_bounds': {
                    'north': cluster_coords[lat_col].max(),
                    'south': cluster_coords[lat_col].min(),
                    'east': cluster_coords[lon_col].max(),
                    'west': cluster_coords[lon_col].min()
                }
            }
            
            if len(cluster_values) > 0:
                avg_performance = cluster_values.mean()
                total_performance = cluster_values.sum()
                
                cluster_info.update({
                    'avg_performance': avg_performance,
                    'total_performance': total_performance,
                    'performance_std': cluster_values.std(),
                    'performance_level': self.classify_performance(avg_performance, df[value_col].mean()),
                    'performance_rank': 0  # Sera calcul√© apr√®s
                })
            
            cluster_analysis.append(cluster_info)
        
        # Classer les clusters par performance
        clusters_with_perf = [c for c in cluster_analysis if 'avg_performance' in c]
        clusters_with_perf.sort(key=lambda x: x['avg_performance'], reverse=True)
        
        for rank, cluster in enumerate(clusters_with_perf, 1):
            cluster['performance_rank'] = rank
        
        return cluster_analysis
    
    def classify_performance(self, value, mean_value):
        """Classifie la performance relative"""
        if value >= mean_value * 1.2:
            return 'Excellent'
        elif value >= mean_value * 1.1:
            return 'High'
        elif value >= mean_value * 0.9:
            return 'Average'
        elif value >= mean_value * 0.8:
            return 'Low'
        else:
            return 'Poor'
    
    def clean_numeric_column(self, series):
        """Nettoyage des colonnes num√©riques"""
        if pd.api.types.is_numeric_dtype(series):
            return series
        
        cleaned = series.astype(str)
        cleaned = cleaned.str.replace(r'[$‚Ç¨¬£¬•‚Çπ‚ÇΩ%]', '', regex=True)
        cleaned = cleaned.str.replace(',', '')
        cleaned = cleaned.str.replace(' ', '')
        cleaned = cleaned.str.replace(r'[^\d.-]', '', regex=True)
        
        return pd.to_numeric(cleaned, errors='coerce')
    
    def generate_geographic_insights(self, df, mappings, clusters):
        """G√©n√©ration d'insights g√©ographiques avanc√©s"""
        insights = []
        recommendations = []
        
        lat_col = mappings['latitude']
        lon_col = mappings['longitude']
        value_col = mappings.get('revenue', list(df.select_dtypes(include=[np.number]).columns)[0])
        
        # Analyse de la dispersion g√©ographique
        spread = self.calculate_geographic_spread(df, lat_col, lon_col)
        
        if spread > 500:
            insights.append(f"üåç **Large couverture territoriale**: {spread:.0f} km d'√©tendue")
            recommendations.append("Consid√©rer la r√©gionalisation de la gestion pour optimiser les op√©rations")
        elif spread > 100:
            insights.append(f"üìç **Couverture r√©gionale**: {spread:.0f} km d'√©tendue")
        else:
            insights.append(f"üèòÔ∏è **Couverture locale**: {spread:.0f} km d'√©tendue")
        
        # Analyse des clusters
        if clusters and len(clusters) > 1:
            high_perf_clusters = [c for c in clusters if c.get('performance_level') in ['Excellent', 'High']]
            low_perf_clusters = [c for c in clusters if c.get('performance_level') in ['Low', 'Poor']]
            
            if high_perf_clusters:
                insights.append(f"üéØ **{len(high_perf_clusters)} zones haute performance** identifi√©es")
                best_cluster = max(high_perf_clusters, key=lambda x: x.get('avg_performance', 0))
                insights.append(f"üíé **Zone leader**: Cluster {best_cluster['cluster_id']+1} avec {best_cluster.get('avg_performance', 0):,.0f} DHS de moyenne")
            
            if low_perf_clusters:
                recommendations.append(f"‚ö° **{len(low_perf_clusters)} zones sous-performantes** n√©cessitent attention")
                recommendations.append("Analyser les facteurs de succ√®s des meilleures zones pour les r√©pliquer")
            
            # Recommandations d'expansion
            if len(high_perf_clusters) >= 1:
                recommendations.append("üöÄ **Opportunit√© d'expansion**: Identifier des zones similaires aux clusters performants")
        
        # Analyse de densit√©
        total_locations = len(df)
        if total_locations > 10:
            avg_distance = self.calculate_average_distance_between_points(df, lat_col, lon_col)
            if avg_distance < 50:
                insights.append(f"üèôÔ∏è **Forte densit√©**: {avg_distance:.0f} km de distance moyenne entre points")
                recommendations.append("Optimiser la cannibalisation entre sites proches")
            else:
                insights.append(f"üåæ **Faible densit√©**: {avg_distance:.0f} km de distance moyenne")
                recommendations.append("Consid√©rer l'ouverture de sites interm√©diaires")
        
        return insights, recommendations
    
    def calculate_geographic_spread(self, df, lat_col, lon_col):
        """Calcule l'√©tendue g√©ographique"""
        if len(df) < 2:
            return 0
        
        max_distance = 0
        coords = df[[lat_col, lon_col]].dropna()
        
        for i in range(len(coords)):
            for j in range(i+1, len(coords)):
                try:
                    coord1 = (coords.iloc[i][lat_col], coords.iloc[i][lon_col])
                    coord2 = (coords.iloc[j][lat_col], coords.iloc[j][lon_col])
                    distance = geodesic(coord1, coord2).kilometers
                    max_distance = max(max_distance, distance)
                except:
                    continue
        
        return max_distance
    
    def calculate_average_distance_between_points(self, df, lat_col, lon_col):
        """Calcule la distance moyenne entre tous les points"""
        coords = df[[lat_col, lon_col]].dropna()
        
        if len(coords) < 2:
            return 0
        
        total_distance = 0
        count = 0
        
        for i in range(len(coords)):
            for j in range(i+1, len(coords)):
                try:
                    coord1 = (coords.iloc[i][lat_col], coords.iloc[i][lon_col])
                    coord2 = (coords.iloc[j][lat_col], coords.iloc[j][lon_col])
                    distance = geodesic(coord1, coord2).kilometers
                    total_distance += distance
                    count += 1
                except:
                    continue
        
        return total_distance / count if count > 0 else 0

# ========== SESSION STATE INITIALIZATION ==========
def init_session_state():
    """Initialisation enrichie des variables de session"""
    
    # Donn√©es CSV enrichies
    if 'csv_data' not in st.session_state:
        st.session_state.csv_data = {}
    
    if 'csv_processor' not in st.session_state:
        st.session_state.csv_processor = EnhancedGeoCSVProcessor()
    
    if 'imported_metrics' not in st.session_state:
        st.session_state.imported_metrics = {}
    
    # G√©n√©rateur de templates enrichi
    if 'template_generator' not in st.session_state:
        st.session_state.template_generator = EnhancedCSVTemplateGenerator()
    
    # Analyseur g√©ographique
    if 'geo_analyzer' not in st.session_state:
        st.session_state.geo_analyzer = GeoFinancialAnalyzer()
    
    # Donn√©es d'analyse
    if 'scenario_results' not in st.session_state:
        st.session_state.scenario_results = {}
    
    if 'ml_forecasts' not in st.session_state:
        st.session_state.ml_forecasts = {}
    
    if 'geographic_analysis' not in st.session_state:
        st.session_state.geographic_analysis = {}

# ========== ENHANCED CSV IMPORT PAGE ==========
def show_enhanced_csv_import():
    """Import CSV enrichi avec g√©olocalisation"""
    st.header("üì§ Import CSV Avanc√© avec G√©olocalisation")
    
    st.markdown("""
    üöÄ **Analyse Financi√®re & G√©ographique Compl√®te**: Uploadez vos donn√©es et obtenez une analyse approfondie 
    incluant la g√©olocalisation, m√©triques business avanc√©es et insights IA !
    
    **Formats Support√©s**: Plus de 30 colonnes d√©tect√©es automatiquement incluant g√©olocalisation
    """)
    
    # Guide du format optimal enrichi
    with st.expander("üìã Guide Complet des Formats CSV", expanded=False):
        st.markdown("""
        ### üéØ Colonnes Recommand√©es pour Analyse Compl√®te
        
        **üìä Donn√©es Financi√®res:**
        - `Date`, `Revenue`, `Sales`, `Costs`, `Profit`, `Cash_Flow`
        
        **üåç Donn√©es G√©ographiques:**
        - `Latitude`, `Longitude`, `Location_Name`, `City`, `Region`, `Country`, `Postal_Code`
        
        **üë• M√©triques Business:**
        - `Customer_Count`, `Customer_Acquisition_Cost`, `Lifetime_Value`, `Churn_Rate`
        - `Market_Share`, `Competition_Level`, `Brand_Strength`
        
        **üìà M√©triques Op√©rationnelles:**
        - `Units_Sold`, `Average_Price`, `Employee_Count`, `Store_Size`
        - `Marketing_Spend`, `Website_Traffic`, `Demographics_Score`
        
        **üí∞ Donn√©es Financi√®res Avanc√©es:**
        - `Assets`, `Liabilities`, `Equity`, `Accounts_Receivable`, `Inventory`
        """)
        
        # Exemple de donn√©es enrichies
        st.markdown("### üìã Exemple de Format Enrichi")
        example_enriched = {
            'Date': ['2025-06-28', '2025-06-28', '2025-06-28'],
            'Location_Name': ['Casablanca_Centre', 'Rabat_Agdal', 'Marrakech_Gueliz'],
            'Latitude': [33.5731, 34.0209, 31.6295],
            'Longitude': [-7.5898, -6.8416, -7.9811],
            'Revenue': [25000, 18000, 15000],
            'Costs': [18750, 13500, 11250],
            'Customer_Count': [250, 180, 150],
            'Market_Share': [0.25, 0.18, 0.15],
            'Competition_Level': [0.7, 0.6, 0.5]
        }
        
        st.dataframe(pd.DataFrame(example_enriched), use_container_width=True)
    
    # Templates enrichis √† t√©l√©charger
    st.markdown("### üì• Templates CSV Enrichis")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("üåç Template G√©ographique Complet", type="primary"):
            template_gen = st.session_state.template_generator
            csv_data = template_gen.generate_template_csv('complete_geo_financial')
            
            if csv_data:
                st.download_button(
                    label="üíæ T√©l√©charger Template Complet",
                    data=csv_data,
                    file_name="template_geo_financier_complet.csv",
                    mime="text/csv"
                )
                st.success("‚úÖ Template complet pr√™t!")
    
    with col2:
        if st.button("üõçÔ∏è Template Retail G√©o"):
            template_gen = st.session_state.template_generator
            csv_data = template_gen.generate_template_csv('retail_geo')
            
            if csv_data:
                st.download_button(
                    label="üíæ T√©l√©charger Template Retail",
                    data=csv_data,
                    file_name="template_retail_geo.csv",
                    mime="text/csv"
                )
    
    with col3:
        if st.button("‚òÅÔ∏è Template SaaS G√©o"):
            template_gen = st.session_state.template_generator
            csv_data = template_gen.generate_template_csv('saas_geo')
            
            if csv_data:
                st.download_button(
                    label="üíæ T√©l√©charger Template SaaS",
                    data=csv_data,
                    file_name="template_saas_geo.csv",
                    mime="text/csv"
                )
    
    # Upload de fichier avec analyse enrichie
    uploaded_file = st.file_uploader(
        "üìÅ Glissez-d√©posez votre fichier CSV enrichi ici",
        type=['csv'],
        help="Supporte fichiers jusqu'√† 200MB avec d√©tection automatique des colonnes g√©ographiques"
    )
    
    if uploaded_file is not None:
        try:
            # Barre de progression enrichie
            progress_bar = st.progress(0)
            status_text = st.empty()
            
            # √âtape 1: Lecture CSV
            status_text.text("üìñ Lecture du fichier CSV...")
            progress_bar.progress(15)
            
            df = pd.read_csv(uploaded_file)
            
            # √âtape 2: D√©tection des colonnes
            status_text.text("üîç D√©tection des colonnes financi√®res et g√©ographiques...")
            progress_bar.progress(30)
            
            processor = st.session_state.csv_processor
            detected_mappings = processor.detect_columns(df)
            
            # √âtape 3: Validation des donn√©es
            status_text.text("‚úÖ Validation de la qualit√© des donn√©es...")
            progress_bar.progress(45)
            
            issues, suggestions = processor.validate_data(df, detected_mappings)
            
            # √âtape 4: Calcul des m√©triques enrichies
            status_text.text("üìä Calcul des m√©triques financi√®res et g√©ographiques...")
            progress_bar.progress(65)
            
            # √âtape 5: Traitement complet
            status_text.text("üß† G√©n√©ration des insights IA et visualisations...")
            progress_bar.progress(85)
            
            results = processor.process_csv(df)
            
            # √âtape 6: Stockage
            status_text.text("üíæ Stockage des r√©sultats d'analyse...")
            progress_bar.progress(95)
            
            st.session_state.csv_data = results
            st.session_state.imported_metrics = results['metrics']
            
            # Finalisation
            status_text.text("üéâ Analyse compl√®te termin√©e!")
            progress_bar.progress(100)
            
            time.sleep(1)
            progress_bar.empty()
            status_text.empty()
            
            # Message de succ√®s enrichi
            st.success(f"üéâ **Analyse Compl√®te R√©ussie!** {len(df)} lignes avec {len(df.columns)} colonnes trait√©es")
            
            # Statistiques d'import enrichies
            col1, col2, col3, col4, col5 = st.columns(5)
            
            with col1:
                st.metric("üìä Lignes", f"{len(df):,}")
            with col2:
                st.metric("üìà Colonnes", len(df.columns))
            with col3:
                detected_cols = len(detected_mappings)
                st.metric("üéØ Auto-D√©tect√©es", detected_cols)
            with col4:
                has_geo = processor.has_geographic_data(detected_mappings, df)
                st.metric("üåç G√©o-Donn√©es", "‚úÖ Oui" if has_geo else "‚ùå Non")
            with col5:
                file_size = uploaded_file.size / (1024 * 1024)
                st.metric("üìÅ Taille", f"{file_size:.1f} MB")
            
            # Affichage des r√©sultats enrichis
            show_enhanced_csv_analysis_results(results)
            
        except Exception as e:
            st.error(f"‚ùå Erreur lors du traitement CSV: {str(e)}")
            st.info("üí° **Guide de D√©pannage:**")
            st.write("‚Ä¢ V√©rifiez que le CSV utilise des virgules comme s√©parateurs")
            st.write("‚Ä¢ Supprimez les symboles mon√©taires ($, ‚Ç¨, etc.)")
            st.write("‚Ä¢ V√©rifiez la coh√©rence des formats de date")
            st.write("‚Ä¢ Les coordonn√©es doivent √™tre en format d√©cimal")
            st.write("‚Ä¢ Assurez-vous que les colonnes num√©riques ne contiennent que des chiffres")
    
    else:
        # Interface d'attente enrichie
        st.markdown("""
        ### üåü Capacit√©s d'Analyse Enrichies:
        
        **üìä Analyse Financi√®re Compl√®te:**
        - Revenus, co√ªts, profits avec tendances temporelles
        - Ratios financiers avanc√©s et benchmarking
        - Calculs de marges et volatilit√© automatiques
        
        **üåç Intelligence G√©ographique:**
        - Cartes de chaleur des performances par zone
        - Clustering automatique des emplacements
        - Analyse de dispersion g√©ographique
        - Corr√©lations g√©o-financi√®res
        
        **üß† Insights IA Avanc√©s:**
        - D√©tection automatique de patterns g√©ographiques
        - Recommandations d'optimisation par zone
        - Alertes de performance g√©o-localis√©es
        - Suggestions d'expansion territoriale
        
        **üìà M√©triques Business Enrichies:**
        - LTV/CAC avec analyse g√©ographique
        - Parts de march√© par r√©gion
        - Analyse concurrentielle localis√©e
        - Scoring d√©mographique des zones
        
        **üîÑ Int√©gration Totale:**
        - Auto-population des mod√®les avanc√©s
        - Synchronisation avec forecasting ML
        - Enrichissement des analyses de risque
        - Dashboard ex√©cutif g√©o-localis√©
        """)

def show_enhanced_csv_analysis_results(results):
    """Affichage enrichi des r√©sultats d'analyse CSV"""
    
    mappings = results['mappings']
    metrics = results['metrics']
    insights_data = results['insights']
    figures = results['figures']
    geographic_analysis = results.get('geographic_analysis')
    df = results['processed_df']
    
    # D√©tection des colonnes enrichie
    st.subheader("üéØ D√©tection Automatique des Colonnes")
    
    if mappings:
        # Organiser par cat√©gories
        financial_cols = {k: v for k, v in mappings.items() if k in ['revenue', 'costs', 'profit', 'cash_flow']}
        geo_cols = {k: v for k, v in mappings.items() if k in ['latitude', 'longitude', 'location_name', 'city', 'region', 'country']}
        business_cols = {k: v for k, v in mappings.items() if k in ['customer_count', 'customer_acquisition_cost', 'lifetime_value', 'market_share']}
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown("#### üí∞ Colonnes Financi√®res")
            for category, column in financial_cols.items():
                st.success(f"**{category.title()}**: `{column}`")
        
        with col2:
            st.markdown("#### üåç Colonnes G√©ographiques")
            for category, column in geo_cols.items():
                st.info(f"**{category.title()}**: `{column}`")
        
        with col3:
            st.markdown("#### üìä Colonnes Business")
            for category, column in business_cols.items():
                st.warning(f"**{category.title()}**: `{column}`")
    
    # Insights IA enrichis
    st.subheader("ü§ñ Intelligence Artificielle - Insights Enrichis")
    
    # Tabs pour diff√©rents types d'insights
    if insights_data:
        insight_tabs = st.tabs(["‚úÖ Insights Cl√©s", "üåç Insights G√©ographiques", "‚ö†Ô∏è Alertes", "üí° Recommandations"])
        
        with insight_tabs[0]:
            if insights_data['insights']:
                for insight in insights_data['insights']:
                    st.success(insight)
            else:
                st.info("Aucun insight sp√©cifique g√©n√©r√© pour le moment.")
        
        with insight_tabs[1]:
            # Insights g√©ographiques sp√©cifiques
            if geographic_analysis:
                geo_insights = generate_geographic_specific_insights(geographic_analysis, metrics)
                for insight in geo_insights:
                    st.info(f"üåç {insight}")
            else:
                st.info("Uploadez des donn√©es avec coordonn√©es pour voir les insights g√©ographiques.")
        
        with insight_tabs[2]:
            if insights_data['alerts']:
                for alert in insights_data['alerts']:
                    st.error(alert)
            else:
                st.success("‚úÖ Aucune alerte critique d√©tect√©e!")
        
        with insight_tabs[3]:
            if insights_data['recommendations']:
                for rec in insights_data['recommendations']:
                    st.warning(f"üí° {rec}")
            else:
                st.info("Aucune recommandation sp√©cifique √† ce stade.")
    
    # Visualisations enrichies
    st.subheader("üìà Visualisations Interactives Enrichies")
    
    if figures:
        # Cr√©er des tabs pour diff√©rents types de viz
        viz_tabs = []
        viz_content = []
        
        if 'financial_trend' in figures:
            viz_tabs.append("üìä Tendances Financi√®res")
            viz_content.append(('financial_trend', figures['financial_trend']))
        
        if 'performance_map' in figures:
            viz_tabs.append("üó∫Ô∏è Carte des Performances")
            viz_content.append(('performance_map', figures['performance_map']))
        
        if 'clusters_map' in figures:
            viz_tabs.append("üéØ Clusters G√©ographiques")
            viz_content.append(('clusters_map', figures['clusters_map']))
        
        if 'business_radar' in figures:
            viz_tabs.append("üì° Radar Business")
            viz_content.append(('business_radar', figures['business_radar']))
        
        if viz_tabs:
            selected_tabs = st.tabs(viz_tabs)
            
            for i, (tab_name, (fig_key, fig)) in enumerate(zip(viz_tabs, viz_content)):
                with selected_tabs[i]:
                    st.plotly_chart(fig, use_container_width=True)
                    
                    # Ajouter des insights sp√©cifiques √† chaque graphique
                    if fig_key == 'performance_map':
                        st.info("üí° **Analyse**: Cette carte montre la r√©partition g√©ographique de vos performances. Les zones plus chaudes indiquent de meilleures performances.")
                    elif fig_key == 'clusters_map':
                        st.info("üí° **Analyse**: Les clusters identifient des groupes d'emplacements avec des caract√©ristiques similaires pour optimiser votre strat√©gie.")
    
    # Analyse g√©ographique d√©taill√©e
    if geographic_analysis:
        st.subheader("üåç Analyse G√©ographique Approfondie")
        
        geo_tabs = st.tabs(["üìä Statistiques G√©o", "üéØ Clustering", "üìà Corr√©lations G√©o-Performance"])
        
        with geo_tabs[0]:
            show_geographic_statistics(geographic_analysis)
        
        with geo_tabs[1]:
            if 'geographic_data' in metrics and 'clusters' in metrics['geographic_data']:
                show_cluster_analysis(metrics['geographic_data']['clusters'])
            else:
                st.info("Clustering g√©ographique non disponible (n√©cessite au moins 3 emplacements)")
        
        with geo_tabs[2]:
            if 'geo_performance_correlation' in geographic_analysis:
                show_geo_performance_correlation(geographic_analysis['geo_performance_correlation'])
            else:
                st.info("Analyse de corr√©lation non disponible")
    
    # Options d'int√©gration enrichies
    st.subheader("üîÑ Options d'Int√©gration Avanc√©es")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        if st.button("üíæ Sauvegarder Analyse", type="primary", use_container_width=True):
            st.success("‚úÖ Analyse compl√®te sauvegard√©e!")
            st.balloons()
    
    with col2:
        if st.button("üß† Analytics Avanc√©s", use_container_width=True):
            st.success("üöÄ Acc√©dez aux Analytics Avanc√©s via le menu...")
            st.info("üëà Utilisez le menu de navigation pour acc√©der aux Analytics Avanc√©s")
    
    with col3:
        if st.button("üåç Analyse G√©ographique", use_container_width=True):
            st.session_state['current_page'] = 'geographic_analysis'
            st.rerun()
    
    with col4:
        if st.button("üéØ Planification Sc√©narios", use_container_width=True):
            st.success("üöÄ Acc√©dez √† la Planification via le menu...")
            st.info("üëà Menu navigation ‚Üí Planification de Sc√©narios")

def generate_geographic_specific_insights(geographic_analysis, metrics):
    """G√©n√®re des insights sp√©cifiques √† l'analyse g√©ographique"""
    insights = []
    
    if 'basic_stats' in geographic_analysis:
        stats = geographic_analysis['basic_stats']
        total_locations = stats.get('total_locations', 0)
        
        if total_locations > 1:
            insights.append(f"Analyse de {total_locations} emplacements g√©ographiques")
            
            bounds = stats.get('geographic_bounds', {})
            if bounds:
                lat_span = bounds.get('north', 0) - bounds.get('south', 0)
                lon_span = bounds.get('east', 0) - bounds.get('west', 0)
                
                if lat_span > 5 or lon_span > 5:
                    insights.append("Large dispersion g√©ographique d√©tect√©e - consid√©rer la r√©gionalisation")
                else:
                    insights.append("Emplacements g√©ographiquement concentr√©s - optimisation logistique possible")
    
    if 'density_analysis' in geographic_analysis:
        density = geographic_analysis['density_analysis']
        avg_distance = density.get('average_distance_between_points', 0)
        
        if avg_distance > 100:
            insights.append(f"Distance moyenne importante entre sites: {avg_distance:.0f} km")
        elif avg_distance > 50:
            insights.append(f"Distance mod√©r√©e entre sites: {avg_distance:.0f} km")
        else:
            insights.append(f"Sites rapproch√©s: {avg_distance:.0f} km de distance moyenne")
    
    if 'geographic_data' in metrics and 'clusters' in metrics['geographic_data']:
        clusters = metrics['geographic_data']['clusters']
        high_perf = [c for c in clusters if c.get('performance_level') in ['Excellent', 'High']]
        
        if high_perf:
            insights.append(f"{len(high_perf)} cluster(s) haute performance identifi√©(s)")
    
    return insights

def show_geographic_statistics(geographic_analysis):
    """Affiche les statistiques g√©ographiques"""
    
    if 'basic_stats' in geographic_analysis:
        stats = geographic_analysis['basic_stats']
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### üìç Statistiques de Base")
            st.metric("Emplacements Totaux", stats.get('total_locations', 0))
            
            center = stats.get('center_point', {})
            if center:
                st.metric("Centre Latitude", f"{center.get('latitude', 0):.4f}¬∞")
                st.metric("Centre Longitude", f"{center.get('longitude', 0):.4f}¬∞")
        
        with col2:
            st.markdown("#### üó∫Ô∏è Limites G√©ographiques")
            bounds = stats.get('geographic_bounds', {})
            if bounds:
                st.metric("Nord", f"{bounds.get('north', 0):.4f}¬∞")
                st.metric("Sud", f"{bounds.get('south', 0):.4f}¬∞")
                st.metric("Est", f"{bounds.get('east', 0):.4f}¬∞")
                st.metric("Ouest", f"{bounds.get('west', 0):.4f}¬∞")
    
    if 'density_analysis' in geographic_analysis:
        density = geographic_analysis['density_analysis']
        
        st.markdown("#### üìä Analyse de Densit√©")
        col1, col2 = st.columns(2)
        
        with col1:
            concentration = density.get('geographic_concentration', 0)
            st.metric("Concentration G√©ographique", f"{concentration:.4f}")
        
        with col2:
            avg_distance = density.get('average_distance_between_points', 0)
            st.metric("Distance Moyenne Entre Points", f"{avg_distance:.1f} km")

def show_cluster_analysis(clusters):
    """Affiche l'analyse des clusters g√©ographiques"""
    
    if not clusters:
        st.info("Aucun cluster g√©ographique d√©tect√©.")
        return
    
    st.markdown(f"#### üéØ {len(clusters)} Cluster(s) G√©ographique(s) Identifi√©(s)")
    
    for i, cluster in enumerate(clusters):
        with st.expander(f"Cluster {i+1} - {cluster.get('performance_level', 'N/A')} Performance"):
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("Emplacements", cluster.get('location_count', 0))
                st.metric("Rayon", f"{cluster.get('radius_km', 0):.1f} km")
            
            with col2:
                st.metric("Centre Latitude", f"{cluster.get('center_lat', 0):.4f}¬∞")
                st.metric("Centre Longitude", f"{cluster.get('center_lon', 0):.4f}¬∞")
            
            with col3:
                if 'avg_performance' in cluster:
                    st.metric("Performance Moyenne", f"{cluster['avg_performance']:,.0f} DHS")
                if 'performance_rank' in cluster:
                    st.metric("Rang Performance", f"#{cluster['performance_rank']}")
            
            # Limites g√©ographiques du cluster
            if 'geographic_bounds' in cluster:
                bounds = cluster['geographic_bounds']
                st.markdown("**Limites G√©ographiques:**")
                st.write(f"Nord: {bounds.get('north', 0):.4f}¬∞ | Sud: {bounds.get('south', 0):.4f}¬∞")
                st.write(f"Est: {bounds.get('east', 0):.4f}¬∞ | Ouest: {bounds.get('west', 0):.4f}¬∞")

def show_geo_performance_correlation(correlation_data):
    """Affiche les corr√©lations g√©o-performance"""
    
    st.markdown("#### üìà Corr√©lations G√©ographiques avec Performance")
    
    col1, col2 = st.columns(2)
    
    with col1:
        lat_corr = correlation_data.get('latitude_correlation', 0)
        st.metric("Corr√©lation Latitude-Performance", f"{lat_corr:.3f}")
        
        if abs(lat_corr) > 0.5:
            st.info("üîç **Forte corr√©lation Nord-Sud** d√©tect√©e")
        elif abs(lat_corr) > 0.3:
            st.info("üìä **Corr√©lation mod√©r√©e Nord-Sud** observ√©e")
    
    with col2:
        lon_corr = correlation_data.get('longitude_correlation', 0)
        st.metric("Corr√©lation Longitude-Performance", f"{lon_corr:.3f}")
        
        if abs(lon_corr) > 0.5:
            st.info("üîç **Forte corr√©lation Est-Ouest** d√©tect√©e")
        elif abs(lon_corr) > 0.3:
            st.info("üìä **Corr√©lation mod√©r√©e Est-Ouest** observ√©e")
    
    # Analyse par quadrants
    if 'quadrant_analysis' in correlation_data:
        st.markdown("#### üó∫Ô∏è Analyse par Quadrants G√©ographiques")
        
        quadrants = correlation_data['quadrant_analysis']
        
        quad_data = []
        for quad_name, quad_info in quadrants.items():
            quad_data.append({
                'Quadrant': quad_name.replace('_', ' ').title(),
                'Emplacements': quad_info.get('count', 0),
                'Revenue Moyenne': f"{quad_info.get('avg_revenue', 0):,.0f} DHS",
                'Revenue Total': f"{quad_info.get('total_revenue', 0):,.0f} DHS"
            })
        
        if quad_data:
            st.dataframe(pd.DataFrame(quad_data), use_container_width=True)

# ========== GEOGRAPHIC ANALYSIS PAGE ==========
def show_geographic_analysis():
    """Page d'analyse g√©ographique compl√®te et avanc√©e"""
    st.header("üåç Analyse G√©ographique Avanc√©e des Performances")
    
    # V√©rification des donn√©es CSV
    csv_data = EnhancedCSVDataManager.get_csv_financial_data()
    
    if not csv_data or 'csv_data' not in st.session_state:
        st.warning("üì§ **Donn√©es g√©ographiques non disponibles**")
        st.info("L'analyse g√©ographique n√©cessite des donn√©es CSV avec coordonn√©es latitude/longitude.")
        
        # Exemple de format g√©ographique
        st.subheader("üìã Format CSV G√©ographique Requis")
        
        example_geo_data = {
            'Date': ['2025-06-28', '2025-06-28', '2025-06-28'],
            'Location_Name': ['Casablanca_Centre', 'Rabat_Agdal', 'Marrakech_Gueliz'],
            'Latitude': [33.5731, 34.0209, 31.6295],
            'Longitude': [-7.5898, -6.8416, -7.9811],
            'Revenue': [25000, 18000, 15000],
            'Customer_Count': [250, 180, 150],
            'Market_Share': [0.25, 0.18, 0.15]
        }
        
        st.dataframe(pd.DataFrame(example_geo_data), use_container_width=True)
        
        col1, col2 = st.columns(2)
        
        with col1:
            # Template g√©ographique
            if st.button("üì• T√©l√©charger Template G√©ographique", type="primary"):
                template_gen = st.session_state.template_generator
                csv_data = template_gen.generate_template_csv('complete_geo_financial')
                
                if csv_data:
                    st.download_button(
                        label="üíæ Template G√©ographique Complet",
                        data=csv_data,
                        file_name="template_geo_analyse.csv",
                        mime="text/csv"
                    )
        
        with col2:
            if st.button("üì§ Retour √† l'Import CSV"):
                st.session_state['current_page'] = 'csv_import'
                st.rerun()
        
        return
    
    # V√©rification des donn√©es g√©ographiques
    df = st.session_state.csv_data['processed_df']
    processor = st.session_state.csv_processor
    geo_mappings = processor.detect_columns(df)
    
    if 'latitude' not in geo_mappings or 'longitude' not in geo_mappings:
        st.error("‚ùå **Coordonn√©es g√©ographiques non d√©tect√©es**")
        
        # Option de g√©ocodage automatique
        if 'location_name' in geo_mappings:
            st.subheader("üîÑ G√©ocodage Automatique Disponible")
            st.info(f"Colonne d'adresses d√©tect√©e: `{geo_mappings['location_name']}`")
            
            if st.button("üåç G√©ocoder les Adresses Automatiquement", type="primary"):
                with st.spinner("G√©ocodage en cours... Cela peut prendre quelques minutes."):
                    try:
                        geocoded_df = processor.geocode_locations(df, geo_mappings['location_name'])
                        
                        if len(geocoded_df) > 0:
                            st.success(f"‚úÖ {len(geocoded_df)} emplacements g√©ocod√©s avec succ√®s!")
                            
                            # Afficher les r√©sultats du g√©ocodage
                            st.subheader("üìç R√©sultats du G√©ocodage")
                            st.dataframe(geocoded_df, use_container_width=True)
                            
                            # Option de t√©l√©chargement du fichier enrichi
                            merged_df = df.merge(geocoded_df, left_on=geo_mappings['location_name'], right_on='location', how='left')
                            
                            csv_enriched = merged_df.to_csv(index=False)
                            st.download_button(
                                label="üíæ T√©l√©charger CSV Enrichi avec Coordonn√©es",
                                data=csv_enriched,
                                file_name="donnees_enrichies_geo.csv",
                                mime="text/csv"
                            )
                        else:
                            st.error("‚ùå Aucune adresse n'a pu √™tre g√©ocod√©e")
                            st.info("üí° V√©rifiez que les noms d'emplacements sont clairs (ex: 'Casablanca, Morocco')")
                    
                    except Exception as e:
                        st.error(f"Erreur lors du g√©ocodage: {str(e)}")
        else:
            st.info("üí° Ajoutez une colonne avec des noms d'emplacements pour le g√©ocodage automatique")
        
        return
    
    st.success("üìç **Donn√©es g√©ographiques d√©tect√©es et valid√©es**")
    
    # Configuration de l'analyse g√©ographique
    st.subheader("‚öôÔ∏è Configuration de l'Analyse G√©ographique")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        # M√©trique √† analyser
        numeric_columns = df.select_dtypes(include=[np.number]).columns.tolist()
        value_column = st.selectbox(
            "üìä M√©trique √† Analyser",
            numeric_columns,
            index=next((i for i, col in enumerate(numeric_columns) if 'revenue' in col.lower()), 0)
        )
    
    with col2:
        # Type de visualisation
        viz_type = st.selectbox(
            "üó∫Ô∏è Type de Visualisation",
            [
                "Carte de Chaleur Interactive", 
                "Carte 3D avec √âl√©vations",
                "Analyse de Clusters",
                "Carte Choropl√®the",
                "Vue d'Ensemble Compl√®te"
            ]
        )
    
    with col3:
        # P√©riode d'analyse
        if 'date' in geo_mappings and geo_mappings['date'] in df.columns:
            df[geo_mappings['date']] = pd.to_datetime(df[geo_mappings['date']], errors='coerce')
            
            date_range = st.date_input(
                "üìÖ P√©riode d'Analyse",
                value=[df[geo_mappings['date']].min().date(), df[geo_mappings['date']].max().date()],
                min_value=df[geo_mappings['date']].min().date(),
                max_value=df[geo_mappings['date']].max().date()
            )
        else:
            date_range = None
            st.info("Pas de filtrage temporel")
    
    with col4:
        # Options avanc√©es
        advanced_options = st.checkbox("üîß Options Avanc√©es", value=False)
        
        if advanced_options:
            cluster_count = st.number_input("Nombre de Clusters", min_value=2, max_value=10, value=3)
            min_cluster_size = st.number_input("Taille Min. Cluster", min_value=1, max_value=10, value=2)
        else:
            cluster_count = 3
            min_cluster_size = 2
    
    # Filtrage des donn√©es par p√©riode
    filtered_df = df.copy()
    if date_range and len(date_range) == 2 and geo_mappings.get('date'):
        start_date, end_date = date_range
        date_col = geo_mappings['date']
        filtered_df = df[
            (df[date_col].dt.date >= start_date) & 
            (df[date_col].dt.date <= end_date)
        ]
    
    # Agr√©gation par localisation
    location_col = geo_mappings.get('location_name')
    
    if location_col and location_col in filtered_df.columns:
        # Agr√©gation par nom d'emplacement
        agg_columns = {
            geo_mappings['latitude']: 'first',
            geo_mappings['longitude']: 'first',
            value_column: 'sum'
        }
        
        # Ajouter d'autres m√©triques disponibles
        for col in ['customer_count', 'market_share', 'competition_level']:
            if col in geo_mappings and geo_mappings[col] in filtered_df.columns:
                agg_columns[geo_mappings[col]] = 'mean'
        
        agg_df = filtered_df.groupby(location_col).agg(agg_columns).reset_index()
        agg_df.columns = [col[0] if isinstance(col, tuple) else col for col in agg_df.columns]
    else:
        # Pas d'agr√©gation possible
        agg_df = filtered_df[[geo_mappings['latitude'], geo_mappings['longitude'], value_column]].dropna()
    
    # M√©triques g√©ographiques
    st.subheader("üìä Aper√ßu de la Performance G√©ographique")
    
    geo_analyzer = st.session_state.geo_analyzer
    
    col1, col2, col3, col4, col5 = st.columns(5)
    
    with col1:
        total_locations = len(agg_df)
        st.metric("üè¢ Emplacements", total_locations)
    
    with col2:
        if total_locations > 1:
            geographic_spread = geo_analyzer.calculate_geographic_spread(
                agg_df, geo_mappings['latitude'], geo_mappings['longitude']
            )
            st.metric("üåç √âtendue", f"{geographic_spread:.0f} km")
        else:
            st.metric("üåç √âtendue", "N/A")
    
    with col3:
        best_location_value = agg_df[value_column].max()
        st.metric("üèÜ Top Performance", f"{best_location_value:,.0f}")
    
    with col4:
        avg_performance = agg_df[value_column].mean()
        st.metric("üìä Moyenne", f"{avg_performance:,.0f}")
    
    with col5:
        performance_std = agg_df[value_column].std()
        cv = (performance_std / avg_performance * 100) if avg_performance > 0 else 0
        st.metric("üìà Variabilit√©", f"{cv:.1f}%")
    
    # G√©n√©ration des visualisations selon le type s√©lectionn√©
    if viz_type == "Carte de Chaleur Interactive":
        st.subheader("üî• Carte de Chaleur Interactive des Performances")
        
        # Carte Folium
        heatmap = geo_analyzer.create_performance_heatmap(
            agg_df, 
            geo_mappings['latitude'], 
            geo_mappings['longitude'], 
            value_column,
            location_col
        )
        
        # Affichage de la carte
        st.components.v1.html(heatmap._repr_html_(), height=650)
        
        # Insights automatiques
        st.info("üí° **Analyse**: Les zones rouges indiquent les plus hautes performances. Utilisez le zoom et les filtres pour explorer en d√©tail.")
        
    elif viz_type == "Carte 3D avec √âl√©vations":
        st.subheader("üèîÔ∏è Visualisation 3D des Performances")
        
        # Carte 3D PyDeck
        try:
            deck_map = geo_analyzer.create_3d_deck_map(
                agg_df,
                geo_mappings['latitude'],
                geo_mappings['longitude'],
                value_column
            )
            
            st.pydeck_chart(deck_map)
            
            st.info("üí° **Analyse**: La hauteur des colonnes repr√©sente la performance. Utilisez la souris pour naviguer en 3D.")
            
        except Exception as e:
            st.error(f"Erreur lors de la cr√©ation de la carte 3D: {str(e)}")
            st.info("Utilisez la carte de chaleur en alternative.")
    
    elif viz_type == "Analyse de Clusters":
        st.subheader("üéØ Analyse Avanc√©e des Clusters G√©ographiques")
        
        # Clustering g√©ographique
        clusters = geo_analyzer.analyze_geographic_clusters(
            agg_df,
            geo_mappings['latitude'],
            geo_mappings['longitude'],
            value_column,
            cluster_count
        )
        
        if clusters:
            # Affichage des r√©sultats de clustering
            st.markdown("### üìã R√©sultats du Clustering")
            
            for i, cluster in enumerate(clusters):
                with st.expander(f"üéØ Cluster {i+1} - {cluster.get('performance_level', 'N/A')} Performance", expanded=i==0):
                    
                    col1, col2, col3 = st.columns(3)
                    
                    with col1:
                        st.metric("üìç Emplacements", cluster.get('location_count', 0))
                        st.metric("üìè Rayon Moyen", f"{cluster.get('radius_km', 0):.1f} km")
                    
                    with col2:
                        st.metric("üó∫Ô∏è Centre Latitude", f"{cluster.get('center_lat', 0):.4f}¬∞")
                        st.metric("üó∫Ô∏è Centre Longitude", f"{cluster.get('center_lon', 0):.4f}¬∞")
                    
                    with col3:
                        if 'avg_performance' in cluster:
                            st.metric("üí∞ Performance Moy.", f"{cluster['avg_performance']:,.0f}")
                        if 'performance_rank' in cluster:
                            st.metric("üèÜ Rang", f"#{cluster['performance_rank']}")
            
            # Carte des clusters
            cluster_map = create_enhanced_cluster_map(agg_df, clusters, geo_mappings, value_column)
            st.plotly_chart(cluster_map, use_container_width=True)
        else:
            st.info("Clustering non disponible avec les donn√©es actuelles.")
    
    elif viz_type == "Carte Choropl√®the":
        st.subheader("üó∫Ô∏è Carte Choropl√®the Avanc√©e")
        
        choropleth_map = geo_analyzer.create_plotly_choropleth_map(
            agg_df,
            geo_mappings['latitude'],
            geo_mappings['longitude'],
            value_column,
            location_col
        )
        
        st.plotly_chart(choropleth_map, use_container_width=True)
        
        st.info("üí° **Analyse**: La taille et couleur des marqueurs repr√©sentent les performances. Survolez pour plus de d√©tails.")
    
    elif viz_type == "Vue d'Ensemble Compl√®te":
        st.subheader("üìä Dashboard G√©ographique Complet")
        
        # Cr√©er un dashboard avec plusieurs visualisations
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### üî• Carte de Chaleur")
            heatmap = geo_analyzer.create_performance_heatmap(
                agg_df, 
                geo_mappings['latitude'], 
                geo_mappings['longitude'], 
                value_column,
                location_col
            )
            st.components.v1.html(heatmap._repr_html_(), height=400)
        
        with col2:
            st.markdown("#### üìä Carte Interactive")
            interactive_map = geo_analyzer.create_plotly_choropleth_map(
                agg_df,
                geo_mappings['latitude'],
                geo_mappings['longitude'],
                value_column,
                location_col
            )
            st.plotly_chart(interactive_map, use_container_width=True, config={'displayModeBar': False})
    
    # Analyse et insights g√©ographiques
    st.subheader("üí° Insights G√©ographiques Automatiques")
    
    # Clustering automatique pour les insights
    clusters = geo_analyzer.analyze_geographic_clusters(
        agg_df,
        geo_mappings['latitude'],
        geo_mappings['longitude'],
        value_column,
        cluster_count
    )
    
    # G√©n√©ration d'insights
    geo_insights, geo_recommendations = geo_analyzer.generate_geographic_insights(agg_df, geo_mappings, clusters)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### üîç Observations Cl√©s")
        if geo_insights:
            for insight in geo_insights:
                st.success(f"‚úÖ {insight}")
        else:
            st.info("Aucun insight g√©ographique sp√©cifique d√©tect√©.")
    
    with col2:
        st.markdown("#### üéØ Recommandations Strat√©giques")
        if geo_recommendations:
            for rec in geo_recommendations:
                st.warning(f"üí° {rec}")
        else:
            st.success("‚úÖ Performance g√©ographique optimale d√©tect√©e!")
    
    # Analyse comparative par zones
    st.subheader("üìà Analyse Comparative par Zones")
    
    if len(agg_df) > 1:
        # Top et bottom performers
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### üèÜ Top Performers")
            top_locations = agg_df.nlargest(min(5, len(agg_df)), value_column)
            
            for idx, location in top_locations.iterrows():
                location_name = location.get(location_col, f"Localisation {idx}") if location_col else f"Point {idx}"
                performance = location[value_column]
                
                # Calcul du pourcentage par rapport √† la moyenne
                perf_vs_avg = ((performance / avg_performance) - 1) * 100 if avg_performance > 0 else 0
                
                st.success(f"üìç **{location_name}**: {performance:,.0f} (+{perf_vs_avg:+.1f}%)")
        
        with col2:
            st.markdown("#### üìâ Zones d'Am√©lioration")
            bottom_locations = agg_df.nsmallest(min(3, len(agg_df)), value_column)
            
            for idx, location in bottom_locations.iterrows():
                location_name = location.get(location_col, f"Localisation {idx}") if location_col else f"Point {idx}"
                performance = location[value_column]
                
                perf_vs_avg = ((performance / avg_performance) - 1) * 100 if avg_performance > 0 else 0
                
                st.error(f"üìç **{location_name}**: {performance:,.0f} ({perf_vs_avg:+.1f}%)")
    
    # M√©triques de corr√©lation g√©ographique
    if len(agg_df) > 3:
        st.subheader("üîó Corr√©lations G√©o-Performance")
        
        # Calculer corr√©lations
        lat_corr = agg_df[geo_mappings['latitude']].corr(agg_df[value_column])
        lon_corr = agg_df[geo_mappings['longitude']].corr(agg_df[value_column])
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("üìè Corr√©lation Latitude", f"{lat_corr:.3f}")
            if abs(lat_corr) > 0.5:
                st.info("üîç Forte influence Nord-Sud")
        
        with col2:
            st.metric("üìè Corr√©lation Longitude", f"{lon_corr:.3f}")
            if abs(lon_corr) > 0.5:
                st.info("üîç Forte influence Est-Ouest")
        
        with col3:
            # Calculer l'index de Moran (autocorr√©lation spatiale) simplifi√©
            moran_index = calculate_simple_moran_index(agg_df, geo_mappings, value_column)
            st.metric("üåê Autocorr√©lation Spatiale", f"{moran_index:.3f}")
            
            if moran_index > 0.3:
                st.info("üîó Clustering spatial d√©tect√©")
            elif moran_index < -0.3:
                st.info("üîÄ Dispersion spatiale d√©tect√©e")
    
    # Export et actions
    st.subheader("üì§ Export et Actions")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        if st.button("üíæ Sauvegarder Analyse G√©o", type="primary"):
            # Sauvegarder les r√©sultats dans session state
            st.session_state['geographic_analysis_results'] = {
                'data': agg_df.to_dict(),
                'clusters': clusters,
                'insights': geo_insights,
                'recommendations': geo_recommendations
            }
            st.success("‚úÖ Analyse g√©ographique sauvegard√©e!")
    
    with col2:
        # Export des donn√©es enrichies
        if st.button("üìä Exporter Donn√©es Enrichies"):
            enriched_data = agg_df.copy()
            
            # Ajouter des m√©triques calcul√©es
            enriched_data['Performance_Rank'] = enriched_data[value_column].rank(method='dense', ascending=False)
            enriched_data['Performance_Percentile'] = enriched_data[value_column].rank(pct=True)
            enriched_data['Vs_Average'] = ((enriched_data[value_column] / avg_performance) - 1) * 100
            
            csv_export = enriched_data.to_csv(index=False)
            st.download_button(
                label="üíæ T√©l√©charger CSV Enrichi",
                data=csv_export,
                file_name=f"analyse_geo_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )
    
    with col3:
        if st.button("üéØ Planification Expansion"):
            st.info("üöÄ Fonctionnalit√© de planification d'expansion en d√©veloppement...")
            # Placeholder pour future fonctionnalit√© d'expansion
    
    with col4:
        if st.button("üìà Analytics Avanc√©s"):
            st.session_state['current_page'] = 'advanced_analytics'
            st.rerun()

def create_enhanced_cluster_map(df, clusters, geo_mappings, value_column):
    """Cr√©er une carte avanc√©e des clusters g√©ographiques"""
    fig = go.Figure()
    
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4', '#FECA57', '#FF9FF3', '#54A0FF']
    
    # Ajouter les points de donn√©es avec couleurs par cluster
    if len(clusters) > 0:
        # Assigner les clusters aux points de donn√©es
        coords = df[[geo_mappings['latitude'], geo_mappings['longitude']]].dropna()
        
        if len(coords) >= 3:
            from sklearn.cluster import KMeans
            kmeans = KMeans(n_clusters=len(clusters), random_state=42, n_init=10)
            cluster_labels = kmeans.fit_predict(coords)
            
            # Ajouter les points par cluster
            for cluster_id in range(len(clusters)):
                cluster_mask = cluster_labels == cluster_id
                cluster_data = df.loc[coords.index[cluster_mask]]
                
                if len(cluster_data) > 0:
                    cluster_color = colors[cluster_id % len(colors)]
                    
                    fig.add_trace(go.Scattermapbox(
                        lat=cluster_data[geo_mappings['latitude']],
                        lon=cluster_data[geo_mappings['longitude']],
                        mode='markers',
                        marker=dict(
                            size=12,
                            color=cluster_color,
                            opacity=0.7
                        ),
                        name=f"Cluster {cluster_id + 1}",
                        text=[f"Cluster {cluster_id + 1}<br>Valeur: {val:,.0f}" 
                              for val in cluster_data[value_column]],
                        hovertemplate='%{text}<extra></extra>'
                    ))
    
    # Ajouter les centres des clusters
    for i, cluster in enumerate(clusters):
        cluster_color = colors[i % len(colors)]
        
        # Centre du cluster
        fig.add_trace(go.Scattermapbox(
            lat=[cluster['center_lat']],
            lon=[cluster['center_lon']],
            mode='markers',
            marker=dict(
                size=20,
                color=cluster_color,
                symbol='star',
                line=dict(width=2, color='white')
            ),
            name=f"Centre Cluster {i+1}",
            text=f"Centre Cluster {i+1}<br>Performance: {cluster.get('performance_level', 'N/A')}<br>Emplacements: {cluster['location_count']}",
            hovertemplate='%{text}<extra></extra>',
            showlegend=False
        ))
        
        # Ajouter un cercle pour repr√©senter le rayon du cluster
        if cluster.get('radius_km', 0) > 0:
            circle_lat, circle_lon = generate_circle_coordinates(
                cluster['center_lat'], 
                cluster['center_lon'], 
                cluster['radius_km']
            )
            
            fig.add_trace(go.Scattermapbox(
                lat=circle_lat,
                lon=circle_lon,
                mode='lines',
                line=dict(width=2, color=cluster_color),
                name=f"Rayon Cluster {i+1}",
                hoverinfo='skip',
                showlegend=False,
                opacity=0.5
            ))
    
    # Configuration de la carte
    fig.update_layout(
        mapbox=dict(
            style='open-street-map',
            center=dict(
                lat=df[geo_mappings['latitude']].mean(),
                lon=df[geo_mappings['longitude']].mean()
            ),
            zoom=6
        ),
        height=600,
        title="Clusters G√©ographiques de Performance",
        legend=dict(
            yanchor="top",
            y=0.99,
            xanchor="left",
            x=0.01
        )
    )
    
    return fig

def generate_circle_coordinates(center_lat, center_lon, radius_km, num_points=50):
    """G√©n√®re les coordonn√©es d'un cercle pour la visualisation"""
    import math
    
    # Conversion approximative km vers degr√©s (d√©pend de la latitude)
    lat_deg_per_km = 1 / 111.0
    lon_deg_per_km = 1 / (111.0 * math.cos(math.radians(center_lat)))
    
    radius_lat = radius_km * lat_deg_per_km
    radius_lon = radius_km * lon_deg_per_km
    
    circle_lat = []
    circle_lon = []
    
    for i in range(num_points + 1):
        angle = 2 * math.pi * i / num_points
        lat = center_lat + radius_lat * math.cos(angle)
        lon = center_lon + radius_lon * math.sin(angle)
        circle_lat.append(lat)
        circle_lon.append(lon)
    
    return circle_lat, circle_lon

def calculate_simple_moran_index(df, geo_mappings, value_column):
    """Calcule un index de Moran simplifi√© pour l'autocorr√©lation spatiale"""
    if len(df) < 3:
        return 0
    
    try:
        from geopy.distance import geodesic
        
        n = len(df)
        coords = df[[geo_mappings['latitude'], geo_mappings['longitude']]].values
        values = df[value_column].values
        
        # Matrice de poids bas√©e sur l'inverse de la distance
        weights = np.zeros((n, n))
        for i in range(n):
            for j in range(n):
                if i != j:
                    try:
                        distance = geodesic(coords[i], coords[j]).kilometers
                        weights[i, j] = 1 / (distance + 1)  # +1 pour √©viter division par 0
                    except:
                        weights[i, j] = 0
        
        # Normaliser les poids
        row_sums = weights.sum(axis=1)
        for i in range(n):
            if row_sums[i] > 0:
                weights[i] = weights[i] / row_sums[i]
        
        # Calculer l'index de Moran
        mean_value = values.mean()
        numerator = 0
        denominator = 0
        
        for i in range(n):
            for j in range(n):
                numerator += weights[i, j] * (values[i] - mean_value) * (values[j] - mean_value)
            denominator += (values[i] - mean_value) ** 2
        
        if denominator > 0:
            moran_index = numerator / denominator
        else:
            moran_index = 0
        
        return moran_index
    
    except Exception:
        return 0

# ========== ENHANCED EXECUTIVE DASHBOARD ==========
def show_enhanced_executive_dashboard():
    """Dashboard ex√©cutif enrichi avec g√©olocalisation"""
    st.header("üëî Dashboard Ex√©cutif Enrichi")
    
    # R√©cup√©ration des donn√©es enrichies
    csv_data = EnhancedCSVDataManager.get_csv_financial_data()
    
    if csv_data:
        st.success("üìä **Dashboard aliment√© par vos donn√©es CSV enrichies**")
        
        # KPIs principaux enrichis
        col1, col2, col3, col4, col5 = st.columns(5)
        
        with col1:
            monthly_revenue = csv_data.get('monthly_revenue', 0)
            st.metric("üí∞ Revenue Mensuel", f"{monthly_revenue:,.0f} DHS")
            
            growth = csv_data.get('revenue_growth', 0)
            if growth > 0:
                st.success(f"üìà +{growth:.1f}%")
            else:
                st.error(f"üìâ {growth:.1f}%")
        
        with col2:
            profit_margin = csv_data.get('profit_margin', 0)
            st.metric("üìä Marge Profit", f"{profit_margin:.1f}%")
            
            if profit_margin > 20:
                st.success("üéØ Excellente")
            elif profit_margin > 10:
                st.info("üìà Bonne")
            else:
                st.warning("‚ö†Ô∏è √Ä am√©liorer")
        
        with col3:
            total_locations = csv_data.get('total_locations', 0)
            st.metric("üè¢ Emplacements", total_locations if total_locations > 0 else "N/A")
            
            if total_locations > 5:
                st.info("üåç Multi-sites")
            elif total_locations > 1:
                st.info("üìç R√©gional")
        
        with col4:
            customer_count = csv_data.get('customer_count', 0)
            st.metric("üë• Clients", f"{customer_count:,.0f}" if customer_count > 0 else "N/A")
            
            ltv_cac = csv_data.get('ltv_cac_ratio', 0)
            if ltv_cac > 3:
                st.success("üíé LTV/CAC > 3")
            elif ltv_cac > 1:
                st.info(f"üìä LTV/CAC: {ltv_cac:.1f}")
        
        with col5:
            market_share = csv_data.get('market_share', 0)
            if market_share > 0:
                st.metric("üéØ Part de March√©", f"{market_share*100:.1f}%")
                
                if market_share > 0.2:
                    st.success("üèÜ Leader")
                elif market_share > 0.1:
                    st.info("üí™ Forte")
                else:
                    st.warning("üìà Croissance")
            else:
                st.metric("üéØ Part de March√©", "N/A")
        
        # Section g√©ographique si disponible
        if csv_data.get('total_locations', 0) > 0:
            st.subheader("üåç Performance G√©ographique")
            
            col1, col2 = st.columns([2, 1])
            
            with col1:
                # Mini carte des performances si donn√©es g√©o disponibles
                if 'csv_data' in st.session_state and st.session_state.csv_data.get('geographic_analysis'):
                    st.markdown("#### üó∫Ô∏è Aper√ßu G√©ographique")
                    
                    # Cr√©er une carte simple pour le dashboard
                    geo_data = st.session_state.csv_data.get('processed_df')
                    processor = st.session_state.csv_processor
                    mappings = processor.detect_columns(geo_data)
                    
                    if 'latitude' in mappings and 'longitude' in mappings:
                        # Carte simple pour dashboard
                        dashboard_map = create_dashboard_mini_map(geo_data, mappings)
                        st.plotly_chart(dashboard_map, use_container_width=True, config={'displayModeBar': False})
                    else:
                        st.info("üìç Coordonn√©es g√©ographiques non disponibles")
                else:
                    # Graphique financier standard
                    csv_figures = EnhancedCSVDataManager.get_csv_visualizations()
                    if csv_figures and 'financial_trend' in csv_figures:
                        st.plotly_chart(csv_figures['financial_trend'], use_container_width=True)
            
            with col2:
                st.markdown("#### üèÜ Top Zones")
                
                # Afficher les meilleures zones si disponible
                best_location = csv_data.get('best_performing_location')
                if best_location:
                    st.success(f"ü•á **{best_location.get('name', 'Zone 1')}**")
                    st.write(f"üí∞ {best_location.get('revenue', 0):,.0f} DHS")
                    
                    # Coordonn√©es si disponibles
                    if 'latitude' in best_location:
                        st.caption(f"üìç {best_location['latitude']:.3f}, {best_location['longitude']:.3f}")
                
                # M√©triques g√©ographiques
                geographic_spread = csv_data.get('geographic_spread', 0)
                if geographic_spread > 0:
                    st.metric("üåç √âtendue", f"{geographic_spread:.0f} km")
                
                # Clusters de performance
                clusters = csv_data.get('geographic_clusters', [])
                if clusters:
                    high_perf_clusters = [c for c in clusters if c.get('performance_level') in ['High', 'Excellent']]
                    st.metric("üéØ Zones Haute Perf.", len(high_perf_clusters))
        
        # Insights ex√©cutifs
        st.subheader("üß† Insights Strat√©giques")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### ‚úÖ Points Forts")
            
            # G√©n√©rer des insights ex√©cutifs automatiques
            exec_insights = generate_executive_insights(csv_data)
            for insight in exec_insights['strengths']:
                st.success(f"‚úÖ {insight}")
        
        with col2:
            st.markdown("#### üéØ Opportunit√©s")
            
            for opportunity in exec_insights['opportunities']:
                st.info(f"üí° {opportunity}")
        
        # Alertes critiques
        if exec_insights['alerts']:
            st.subheader("‚ö†Ô∏è Alertes Strat√©giques")
            for alert in exec_insights['alerts']:
                st.error(f"üö® {alert}")
        
        # Actions recommand√©es
        st.subheader("üéØ Actions Prioritaires")
        
        recommended_actions = generate_executive_actions(csv_data)
        for i, action in enumerate(recommended_actions, 1):
            st.warning(f"**Action {i}**: {action}")
    
    else:
        # Pas de donn√©es CSV
        st.warning("üì§ **Aucune donn√©e CSV import√©e**")
        st.info("Importez vos donn√©es financi√®res via Smart CSV Import pour voir le dashboard complet!")
        
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("üì§ Importer Donn√©es CSV", type="primary", use_container_width=True):
                st.session_state['current_page'] = 'csv_import'
                st.rerun()
        
        with col2:
            st.markdown("""
            **Dashboard Ex√©cutif Enrichi:**
            - KPIs financiers en temps r√©el
            - Performance g√©ographique
            - Insights IA automatiques
            - Alertes strat√©giques
            - Recommandations d'actions
            """)

def create_dashboard_mini_map(df, mappings):
    """Cr√©e une mini-carte pour le dashboard ex√©cutif"""
    
    lat_col = mappings['latitude']
    lon_col = mappings['longitude']
    
    # Utiliser les revenus comme m√©trique par d√©faut
    value_col = 'revenue'
    if 'revenue' in mappings:
        value_col = mappings['revenue']
    else:
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        value_col = numeric_cols[0] if len(numeric_cols) > 0 else lat_col
    
    # Nettoyer les donn√©es
    plot_df = df[[lat_col, lon_col, value_col]].dropna()
    
    if len(plot_df) == 0:
        # Retourner une carte vide
        fig = go.Figure()
        fig.update_layout(height=300, title="Aucune donn√©e g√©ographique")
        return fig
    
    # Calculer les tailles de marqueurs
    if plot_df[value_col].max() > 0:
        marker_sizes = (plot_df[value_col] / plot_df[value_col].max() * 30 + 10)
    else:
        marker_sizes = [15] * len(plot_df)
    
    fig = go.Figure()
    
    fig.add_trace(go.Scattermapbox(
        lat=plot_df[lat_col],
        lon=plot_df[lon_col],
        mode='markers',
        marker=dict(
            size=marker_sizes,
            color=plot_df[value_col],
            colorscale='Viridis',
            opacity=0.7,
            showscale=False
        ),
        text=[f"Performance: {val:,.0f}" for val in plot_df[value_col]],
        hovertemplate='%{text}<extra></extra>',
        showlegend=False
    ))
    
    fig.update_layout(
        mapbox=dict(
            style='open-street-map',
            center=dict(
                lat=plot_df[lat_col].mean(),
                lon=plot_df[lon_col].mean()
            ),
            zoom=5
        ),
        height=300,
        margin=dict(l=0, r=0, t=0, b=0)
    )
    
    return fig

def generate_executive_insights(csv_data):
    """G√©n√®re des insights pour le niveau ex√©cutif"""
    insights = {
        'strengths': [],
        'opportunities': [],
        'alerts': []
    }
    
    # Analyse des forces
    profit_margin = csv_data.get('profit_margin', 0)
    revenue_growth = csv_data.get('revenue_growth', 0)
    total_locations = csv_data.get('total_locations', 0)
    market_share = csv_data.get('market_share', 0)
    
    if profit_margin > 15:
        insights['strengths'].append(f"Marges excellentes de {profit_margin:.1f}% - bien au-dessus de la moyenne sectorielle")
    
    if revenue_growth > 10:
        insights['strengths'].append(f"Croissance forte de {revenue_growth:.1f}% - momentum positif confirm√©")
    
    if total_locations > 5:
        insights['strengths'].append(f"Pr√©sence g√©ographique √©tendue avec {total_locations} emplacements")
    
    if market_share > 0.15:
        insights['strengths'].append(f"Position de leader avec {market_share*100:.1f}% de part de march√©")
    
    # Analyse des opportunit√©s
    if revenue_growth < 5:
        insights['opportunities'].append("Acc√©l√©ration de la croissance via expansion g√©ographique ou nouveaux produits")
    
    if profit_margin < 10:
        insights['opportunities'].append("Optimisation des marges par am√©lioration op√©rationnelle")
    
    if total_locations > 0:
        best_location = csv_data.get('best_performing_location')
        if best_location:
            insights['opportunities'].append(f"R√©pliquer le mod√®le de {best_location.get('name', 'la meilleure zone')} sur d'autres sites")
    
    # Alertes critiques
    if profit_margin < 5:
        insights['alerts'].append("Marges critiquement faibles - action imm√©diate requise")
    
    if revenue_growth < -5:
        insights['alerts'].append("D√©clin de revenus d√©tect√© - plan de redressement urgent")
    
    ltv_cac = csv_data.get('ltv_cac_ratio', 0)
    if 0 < ltv_cac < 2:
        insights['alerts'].append("Ratio LTV/CAC dangereux - rentabilit√© client menac√©e")
    
    return insights

def generate_executive_actions(csv_data):
    """G√©n√®re des actions prioritaires pour l'ex√©cutif"""
    actions = []
    
    profit_margin = csv_data.get('profit_margin', 0)
    revenue_growth = csv_data.get('revenue_growth', 0)
    total_locations = csv_data.get('total_locations', 0)
    
    # Actions bas√©es sur les m√©triques
    if profit_margin < 10:
        actions.append("Lancer un audit des co√ªts et identifier 15% de r√©ductions possibles")
    
    if revenue_growth < 5:
        actions.append("D√©velopper un plan d'expansion sur 3 nouveaux march√©s prioritaires")
    
    if total_locations > 3:
        # Actions g√©ographiques
        clusters = csv_data.get('geographic_clusters', [])
        low_perf_clusters = [c for c in clusters if c.get('performance_level') in ['Low', 'Poor']]
        
        if low_perf_clusters:
            actions.append(f"Optimiser {len(low_perf_clusters)} zone(s) sous-performante(s) identifi√©e(s)")
    
    # Actions business intelligence
    if csv_data.get('customer_count', 0) > 0:
        actions.append("Impl√©menter un syst√®me de scoring client pour optimiser l'acquisition")
    
    # Actions par d√©faut si aucune action sp√©cifique
    if not actions:
        actions.append("Poursuivre la strat√©gie actuelle avec monitoring renforc√© des KPIs")
        actions.append("Pr√©parer les plans de scaling pour capitaliser sur la performance")
    
    return actions[:3]  # Limiter √† 3 actions prioritaires

# ========== ENHANCED ADVANCED ANALYTICS ==========
def show_enhanced_advanced_analytics():
    """Analytics avanc√©s enrichis avec g√©olocalisation"""
    st.header("üß† Analytics Avanc√©s avec Intelligence G√©ographique")
    
    # R√©cup√©ration des donn√©es enrichies
    csv_data = EnhancedCSVDataManager.get_csv_financial_data()
    
    if not csv_data:
        st.warning("üì§ **Aucune donn√©e CSV disponible**")
        st.info("Les Analytics Avanc√©s n√©cessitent vos donn√©es CSV pour des analyses approfondies.")
        
        col1, col2 = st.columns(2)
        
        with col1:
            if st.button("üì§ Importer Donn√©es CSV", type="primary", use_container_width=True):
                st.session_state['current_page'] = 'csv_import'
                st.rerun()
        
        with col2:
            st.markdown("""
            **Analytics Avanc√©s incluront:**
            - Scoring de sant√© financi√®re multicrit√®res
            - Corr√©lations g√©o-financi√®res
            - Benchmarking sectoriel avanc√©
            - Pr√©dictions bas√©es sur l'IA
            - Optimisation g√©ographique
            """)
        return
    
    st.success("üìä **Analytics aliment√©s par vos donn√©es enrichies**")
    
    # Score de sant√© financi√®re enrichi
    st.subheader("üéØ Score de Sant√© Financi√®re Enrichi")
    
    health_score, score_breakdown = calculate_enhanced_health_score(csv_data)
    
    col1, col2, col3, col4, col5 = st.columns(5)
    
    with col1:
        st.metric("Score Global", f"{health_score:.0f}/100")
        
        if health_score >= 85:
            st.success("üü¢ Excellent")
        elif health_score >= 70:
            st.info("üîµ Bon")
        elif health_score >= 50:
            st.warning("üü° Moyen")
        else:
            st.error("üî¥ Faible")
    
    with col2:
        liquidity_score = score_breakdown.get('liquidity', 0)
        st.metric("Liquidit√©", f"{liquidity_score:.0f}/25")
        
        current_ratio = csv_data.get('current_ratio', 0)
        if current_ratio > 1.5:
            st.success("üíß Saine")
        elif current_ratio > 1.2:
            st.info("üìä Correcte")
        else:
            st.warning("‚ö†Ô∏è Tendue")
    
    with col3:
        profitability_score = score_breakdown.get('profitability', 0)
        st.metric("Rentabilit√©", f"{profitability_score:.0f}/35")
        
        profit_margin = csv_data.get('profit_margin', 0)
        if profit_margin > 15:
            st.success("üí∞ Forte")
        elif profit_margin > 8:
            st.info("üìà Bonne")
        else:
            st.warning("üìâ Faible")
    
    with col4:
        geographic_score = score_breakdown.get('geographic', 0)
        st.metric("G√©ographique", f"{geographic_score:.0f}/20")
        
        total_locations = csv_data.get('total_locations', 0)
        if total_locations > 5:
            st.success("üåç Diversifi√©")
        elif total_locations > 1:
            st.info("üìç R√©gional")
        else:
            st.warning("üè¢ Local")
    
    with col5:
        business_score = score_breakdown.get('business', 0)
        st.metric("Business", f"{business_score:.0f}/20")
        
        ltv_cac = csv_data.get('ltv_cac_ratio', 0)
        if ltv_cac > 3:
            st.success("üéØ Optimal")
        elif ltv_cac > 1:
            st.info("üìä Correct")
        else:
            st.warning("‚ö†Ô∏è Risqu√©")
    
    # Analyse d√©taill√©e par onglets
    tab1, tab2, tab3, tab4 = st.tabs([
        "üìä Performance Enrichie", 
        "üåç Intelligence G√©ographique", 
        "ü§ñ IA & Pr√©dictions",
        "üìà Optimisation & Recommandations"
    ])
    
    with tab1:
        show_enhanced_performance_analysis(csv_data)
    
    with tab2:
        show_geographic_intelligence(csv_data)
    
    with tab3:
        show_ai_predictions(csv_data)
    
    with tab4:
        show_optimization_recommendations(csv_data)

def calculate_enhanced_health_score(csv_data):
    """Calcule un score de sant√© financi√®re enrichi incluant g√©ographie"""
    scores = {}
    
    # Score de liquidit√© (25 points)
    current_ratio = csv_data.get('current_ratio', 1.5)
    liquidity_score = min(25, current_ratio * 16.67)  # 1.5 ratio = 25 points
    scores['liquidity'] = liquidity_score
    
    # Score de rentabilit√© (35 points)
    profit_margin = csv_data.get('profit_margin', 0)
    net_margin = csv_data.get('net_margin', 0)
    
    margin_score = min(25, profit_margin * 1.67)  # 15% margin = 25 points
    efficiency_score = min(10, net_margin * 100)  # 10% net margin = 10 points
    scores['profitability'] = margin_score + efficiency_score
    
    # Score g√©ographique (20 points) - NOUVEAU
    total_locations = csv_data.get('total_locations', 0)
    geographic_spread = csv_data.get('geographic_spread', 0)
    
    location_score = min(10, total_locations * 2)  # 5 locations = 10 points
    spread_score = min(10, geographic_spread / 50)  # 500km = 10 points
    scores['geographic'] = location_score + spread_score
    
    # Score business (20 points) - NOUVEAU
    ltv_cac = csv_data.get('ltv_cac_ratio', 0)
    market_share = csv_data.get('market_share', 0)
    
    ltv_score = min(15, ltv_cac * 5)  # Ratio 3 = 15 points
    market_score = min(5, market_share * 25)  # 20% = 5 points
    scores['business'] = ltv_score + market_score
    
    total_score = sum(scores.values())
    
    return min(100, total_score), scores

def show_enhanced_performance_analysis(csv_data):
    """Analyse de performance enrichie"""
    st.markdown("### üìä Analyse de Performance Multi-dimensionnelle")
    
    # M√©triques temporelles si disponibles
    if 'revenue_data' in csv_data and csv_data['revenue_data']:
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### üìà √âvolution Temporelle")
            
            revenue_data = csv_data['revenue_data']
            profit_data = csv_data.get('profit_data', [])
            
            # Graphique d'√©volution
            months = list(range(1, len(revenue_data) + 1))
            
            fig = go.Figure()
            
            fig.add_trace(go.Scatter(
                x=months,
                y=revenue_data,
                mode='lines+markers',
                name='Revenue',
                line=dict(color='green', width=3)
            ))
            
            if profit_data:
                fig.add_trace(go.Scatter(
                    x=months[:len(profit_data)],
                    y=profit_data,
                    mode='lines+markers',
                    name='Profit',
                    line=dict(color='blue', width=3)
                ))
            
            fig.update_layout(
                title="√âvolution Revenue & Profit",
                xaxis_title="Mois",
                yaxis_title="Montant (DHS)",
                height=400
            )
            
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            st.markdown("#### üìä M√©triques de Volatilit√©")
            
            revenue_volatility = csv_data.get('revenue_volatility', 0)
            revenue_growth = csv_data.get('revenue_growth', 0)
            
            # Gauge de volatilit√©
            fig_gauge = go.Figure(go.Indicator(
                mode="gauge+number",
                value=revenue_volatility * 100,
                domain={'x': [0, 1], 'y': [0, 1]},
                title={'text': "Volatilit√© Revenue (%)"},
                gauge={
                    'axis': {'range': [None, 50]},
                    'bar': {'color': "darkblue"},
                    'steps': [
                        {'range': [0, 10], 'color': "lightgreen"},
                        {'range': [10, 25], 'color': "yellow"},
                        {'range': [25, 50], 'color': "red"}
                    ],
                    'threshold': {
                        'line': {'color': "red", 'width': 4},
                        'thickness': 0.75,
                        'value': 30
                    }
                }
            ))
            
            fig_gauge.update_layout(height=300)
            st.plotly_chart(fig_gauge, use_container_width=True)
            
            # M√©triques additionnelles
            st.metric("Croissance Revenue", f"{revenue_growth:+.1f}%")
            
            trend = "Croissante" if revenue_growth > 0 else "D√©croissante"
            st.metric("Tendance", trend)
    
    # Analyse comparative sectorielle
    st.markdown("#### üè≠ Benchmarking Sectoriel Intelligent")
    
    # D√©tection automatique du secteur
    sector = detect_business_sector(csv_data)
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.info(f"üéØ **Secteur D√©tect√©**: {sector}")
        
        # Benchmarks sectoriels
        benchmarks = get_sector_benchmarks(sector)
        
        st.markdown("**Benchmarks Secteur:**")
        for metric, value in benchmarks.items():
            if isinstance(value, float) and value < 1:
                st.write(f"‚Ä¢ {metric}: {value:.1%}")
            else:
                st.write(f"‚Ä¢ {metric}: {value:.1f}")
    
    with col2:
        st.markdown("**Votre Performance:**")
        
        profit_margin = csv_data.get('profit_margin', 0) / 100
        market_share = csv_data.get('market_share', 0)
        revenue_growth = csv_data.get('revenue_growth', 0) / 100
        
        company_metrics = {
            'Marge Profit': profit_margin,
            'Part March√©': market_share,
            'Croissance': revenue_growth
        }
        
        for metric, value in company_metrics.items():
            if value > 0:
                if metric == 'Croissance':
                    st.write(f"‚Ä¢ {metric}: {value:.1%}")
                else:
                    st.write(f"‚Ä¢ {metric}: {value:.1%}")
    
    with col3:
        st.markdown("**√âcart vs Secteur:**")
        
        # Comparaison automatique
        comparison = compare_to_sector(csv_data, benchmarks)
        
        for metric, gap in comparison.items():
            color = "üü¢" if gap > 0 else "üî¥" if gap < -10 else "üü°"
            st.write(f"‚Ä¢ {color} {metric}: {gap:+.1f}%")

def detect_business_sector(csv_data):
    """D√©tecte automatiquement le secteur d'activit√©"""
    
    profit_margin = csv_data.get('profit_margin', 0)
    total_locations = csv_data.get('total_locations', 0)
    ltv_cac = csv_data.get('ltv_cac_ratio', 0)
    revenue_volatility = csv_data.get('revenue_volatility', 0)
    
    # Logique de d√©tection bas√©e sur les patterns
    if ltv_cac > 3 and profit_margin > 15:
        return "SaaS/Technologie"
    elif total_locations > 5 and revenue_volatility > 0.2:
        return "Commerce/Retail"
    elif profit_margin < 10 and total_locations > 3:
        return "Manufacturing"
    elif profit_margin > 15 and total_locations <= 2:
        return "Services Professionnels"
    else:
        return "G√©n√©ral"

def get_sector_benchmarks(sector):
    """Retourne les benchmarks sectoriels"""
    
    benchmarks_db = {
        'SaaS/Technologie': {
            'Marge Profit': 0.20,
            'Croissance': 0.25,
            'LTV/CAC': 4.0
        },
        'Commerce/Retail': {
            'Marge Profit': 0.05,
            'Croissance': 0.08,
            'Rotation Stock': 6.0
        },
        'Manufacturing': {
            'Marge Profit': 0.08,
            'Croissance': 0.06,
            'Utilisation Capacit√©': 0.85
        },
        'Services Professionnels': {
            'Marge Profit': 0.18,
            'Croissance': 0.12,
            'Utilisation': 0.75
        },
        'G√©n√©ral': {
            'Marge Profit': 0.12,
            'Croissance': 0.10,
            'ROI': 0.15
        }
    }
    
    return benchmarks_db.get(sector, benchmarks_db['G√©n√©ral'])

def compare_to_sector(csv_data, benchmarks):
    """Compare les m√©triques de l'entreprise aux benchmarks sectoriels"""
    
    comparison = {}
    
    # Marge profit
    company_margin = csv_data.get('profit_margin', 0) / 100
    sector_margin = benchmarks.get('Marge Profit', 0.1)
    
    margin_gap = ((company_margin - sector_margin) / sector_margin) * 100
    comparison['Marge Profit'] = margin_gap
    
    # Croissance
    company_growth = csv_data.get('revenue_growth', 0) / 100
    sector_growth = benchmarks.get('Croissance', 0.1)
    
    growth_gap = ((company_growth - sector_growth) / sector_growth) * 100
    comparison['Croissance'] = growth_gap
    
    # LTV/CAC si disponible
    ltv_cac = csv_data.get('ltv_cac_ratio', 0)
    sector_ltv_cac = benchmarks.get('LTV/CAC', 0)
    
    if ltv_cac > 0 and sector_ltv_cac > 0:
        ltv_gap = ((ltv_cac - sector_ltv_cac) / sector_ltv_cac) * 100
        comparison['LTV/CAC'] = ltv_gap
    
    return comparison

def show_geographic_intelligence(csv_data):
    """Affiche l'intelligence g√©ographique avanc√©e"""
    st.markdown("### üåç Intelligence G√©ographique Avanc√©e")
    
    total_locations = csv_data.get('total_locations', 0)
    
    if total_locations == 0:
        st.info("üìç Aucune donn√©e g√©ographique disponible. Uploadez un CSV avec coordonn√©es pour acc√©der √† l'intelligence g√©ographique.")
        return
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### üéØ Clusters de Performance")
        
        clusters = csv_data.get('geographic_clusters', [])
        
        if clusters:
            # Analyse des clusters
            high_perf = [c for c in clusters if c.get('performance_level') in ['High', 'Excellent']]
            low_perf = [c for c in clusters if c.get('performance_level') in ['Low', 'Poor']]
            
            st.metric("Clusters Haute Performance", len(high_perf))
            st.metric("Clusters Sous-performants", len(low_perf))
            
            # Recommandations bas√©es sur clusters
            if high_perf and low_perf:
                st.success("üí° **Opportunit√©**: R√©pliquer les strat√©gies des zones performantes")
            elif high_perf:
                st.info("üéØ **Strat√©gie**: √âtendre le mod√®le des zones performantes")
            
            # D√©tails des meilleurs clusters
            if high_perf:
                best_cluster = max(high_perf, key=lambda x: x.get('avg_performance', 0))
                st.markdown("**üèÜ Meilleur Cluster:**")
                st.write(f"‚Ä¢ Performance: {best_cluster.get('avg_performance', 0):,.0f} DHS")
                st.write(f"‚Ä¢ Emplacements: {best_cluster.get('location_count', 0)}")
                st.write(f"‚Ä¢ Rayon: {best_cluster.get('radius_km', 0):.1f} km")
        else:
            st.info("Clustering g√©ographique en cours d'analyse...")
    
    with col2:
        st.markdown("#### üìä M√©triques G√©ographiques")
        
        geographic_spread = csv_data.get('geographic_spread', 0)
        st.metric("√âtendue G√©ographique", f"{geographic_spread:.0f} km")
        
        if geographic_spread > 500:
            st.warning("‚ö†Ô∏è **Large dispersion** - Consid√©rer la r√©gionalisation")
        elif geographic_spread > 100:
            st.info("üìç **Pr√©sence r√©gionale** - Optimisation logistique possible")
        else:
            st.success("üèòÔ∏è **Concentration locale** - Synergie g√©ographique")
        
        # Analyse de densit√©
        if total_locations > 1:
            density = total_locations / (geographic_spread + 1)
            st.metric("Densit√©", f"{density:.2f} sites/100km")
            
            if density > 1:
                st.success("üéØ **Forte densit√©** - Couverture optimale")
            else:
                st.info("üìç **Expansion possible** - Zones interm√©diaires")
    
    # Corr√©lations g√©ographiques
    st.markdown("#### üîó Corr√©lations G√©o-Performance")
    
    # Simuler des corr√©lations bas√©es sur les donn√©es disponibles
    if 'best_performing_location' in csv_data:
        best_loc = csv_data['best_performing_location']
        
        st.markdown("**Facteurs de Succ√®s G√©ographique:**")
        
        # Analyse pattern g√©ographique
        geographic_insights = analyze_geographic_patterns(csv_data)
        
        for insight in geographic_insights:
            st.info(f"üìä {insight}")

def analyze_geographic_patterns(csv_data):
    """Analyse les patterns g√©ographiques"""
    insights = []
    
    best_location = csv_data.get('best_performing_location', {})
    geographic_spread = csv_data.get('geographic_spread', 0)
    total_locations = csv_data.get('total_locations', 0)
    
    if best_location:
        insights.append(f"La zone {best_location.get('name', 'leader')} g√©n√®re {best_location.get('revenue', 0):,.0f} DHS")
    
    if geographic_spread > 200 and total_locations > 3:
        insights.append("Dispersion g√©ographique importante - potentiel de r√©gionalisation")
    
    if total_locations > 5:
        insights.append("Pr√©sence multi-sites √©tablie - synergie inter-zones possible")
    
    clusters = csv_data.get('geographic_clusters', [])
    if clusters:
        high_perf_clusters = [c for c in clusters if c.get('performance_level') in ['High', 'Excellent']]
        if len(high_perf_clusters) > 1:
            insights.append(f"{len(high_perf_clusters)} zones d'excellence identifi√©es - mod√®le r√©plicable")
    
    return insights[:3]  # Limiter √† 3 insights

def show_ai_predictions(csv_data):
    """Affiche les pr√©dictions IA"""
    st.markdown("### ü§ñ Pr√©dictions & Intelligence Artificielle")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("#### üîÆ Pr√©dictions 6 mois")
        
        # Pr√©dictions bas√©es sur les tendances actuelles
        revenue_growth = csv_data.get('revenue_growth', 0)
        current_revenue = csv_data.get('monthly_revenue', 0)
        
        # Pr√©diction simple bas√©e sur la croissance
        if revenue_growth != 0:
            predicted_revenue = current_revenue * (1 + revenue_growth/100) ** 6
            
            st.metric("Revenue Pr√©dit (6 mois)", f"{predicted_revenue:,.0f} DHS")
            
            confidence = calculate_prediction_confidence(csv_data)
            st.metric("Confiance Pr√©diction", f"{confidence:.0f}%")
            
            if confidence > 80:
                st.success("üéØ **Haute confiance** - Tendance stable")
            elif confidence > 60:
                st.info("üìä **Confiance mod√©r√©e** - Surveillance recommand√©e")
            else:
                st.warning("‚ö†Ô∏è **Faible confiance** - Volatilit√© √©lev√©e")
        else:
            st.info("Donn√©es insuffisantes pour pr√©dictions fiables")
    
    with col2:
        st.markdown("#### üß† Insights IA")
        
        # G√©n√©ration d'insights IA
        ai_insights = generate_ai_insights(csv_data)
        
        for insight in ai_insights:
            st.info(f"ü§ñ {insight}")
    
    # Analyse de risques pr√©dictive
    st.markdown("#### ‚ö†Ô∏è Analyse de Risques Pr√©dictive")
    
    risk_factors = analyze_predictive_risks(csv_data)
    
    if risk_factors['high']:
        st.error("üö® **Risques √âlev√©s D√©tect√©s:**")
        for risk in risk_factors['high']:
            st.error(f"‚Ä¢ {risk}")
    
    if risk_factors['medium']:
        st.warning("‚ö†Ô∏è **Risques Mod√©r√©s:**")
        for risk in risk_factors['medium']:
            st.warning(f"‚Ä¢ {risk}")
    
    if risk_factors['low']:
        st.info("üìä **Points d'Attention:**")
        for risk in risk_factors['low']:
            st.info(f"‚Ä¢ {risk}")
    
    if not any(risk_factors.values()):
        st.success("‚úÖ **Profil de risque optimal** - Aucun risque majeur d√©tect√©")

def calculate_prediction_confidence(csv_data):
    """Calcule la confiance dans les pr√©dictions"""
    
    confidence = 100
    
    # R√©duire selon la volatilit√©
    volatility = csv_data.get('revenue_volatility', 0)
    confidence -= volatility * 200  # Volatilit√© de 0.3 = -60 points
    
    # R√©duire selon l'√¢ge des donn√©es
    revenue_data = csv_data.get('revenue_data', [])
    if len(revenue_data) < 6:
        confidence -= 20  # Donn√©es insuffisantes
    
    # R√©duire selon la tendance
    revenue_growth = csv_data.get('revenue_growth', 0)
    if abs(revenue_growth) > 50:  # Croissance extr√™me
        confidence -= 30
    
    return max(0, min(100, confidence))

def generate_ai_insights(csv_data):
    """G√©n√®re des insights IA automatiques"""
    insights = []
    
    profit_margin = csv_data.get('profit_margin', 0)
    revenue_growth = csv_data.get('revenue_growth', 0)
    total_locations = csv_data.get('total_locations', 0)
    ltv_cac = csv_data.get('ltv_cac_ratio', 0)
    
    # Pattern recognition
    if profit_margin > 15 and revenue_growth > 10:
        insights.append("Mod√®le √©conomique solide d√©tect√© - phase de croissance rentable")
    
    if total_locations > 3 and profit_margin > 10:
        insights.append("Scalabilit√© g√©ographique valid√©e - potentiel d'expansion confirm√©")
    
    if ltv_cac > 3 and revenue_growth > 0:
        insights.append("Unit economics saines - mod√®le d'acquisition client viable")
    
    # D√©tection d'anomalies
    revenue_volatility = csv_data.get('revenue_volatility', 0)
    if revenue_volatility > 0.3:
        insights.append("Volatilit√© √©lev√©e d√©tect√©e - diversification des revenus recommand√©e")
    
    if profit_margin < 5 and revenue_growth > 15:
        insights.append("Croissance non-rentable identifi√©e - optimisation des co√ªts prioritaire")
    
    return insights[:4]  # Limiter √† 4 insights

def analyze_predictive_risks(csv_data):
    """Analyse les risques de mani√®re pr√©dictive"""
    
    risks = {'high': [], 'medium': [], 'low': []}
    
    profit_margin = csv_data.get('profit_margin', 0)
    revenue_growth = csv_data.get('revenue_growth', 0)
    current_ratio = csv_data.get('current_ratio', 1.5)
    revenue_volatility = csv_data.get('revenue_volatility', 0)
    
    # Risques √©lev√©s
    if profit_margin < 3:
        risks['high'].append("Marges critiquement faibles - risque de faillite")
    
    if current_ratio < 1:
        risks['high'].append("Liquidit√© critique - incapacit√© √† honorer les dettes")
    
    if revenue_growth < -20:
        risks['high'].append("D√©clin s√©v√®re des revenus - plan de sauvetage requis")
    
    # Risques mod√©r√©s
    if profit_margin < 8:
        risks['medium'].append("Marges faibles - vuln√©rabilit√© aux chocs externes")
    
    if revenue_volatility > 0.4:
        risks['medium'].append("Forte volatilit√© - pr√©visibilit√© compromise")
    
    if revenue_growth < 0:
        risks['medium'].append("D√©clin des revenus - investigation requise")
    
    # Points d'attention
    if current_ratio < 1.3:
        risks['low'].append("Liquidit√© tendue - surveillance du cash flow")
    
    ltv_cac = csv_data.get('ltv_cac_ratio', 0)
    if 0 < ltv_cac < 2:
        risks['low'].append("Unit economics limites - optimisation CAC/LTV")
    
    return risks

def show_optimization_recommendations(csv_data):
    """Affiche les recommandations d'optimisation"""
    st.markdown("### üéØ Optimisation & Recommandations Strat√©giques")
    
    # Recommandations par priorit√©
    high_priority, medium_priority, long_term = generate_strategic_recommendations(csv_data)
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.markdown("#### üö® Priorit√© Haute (0-30 jours)")
        
        for rec in high_priority:
            st.error(f"üî• {rec}")
    
    with col2:
        st.markdown("#### ‚ö†Ô∏è Priorit√© Moyenne (1-3 mois)")
        
        for rec in medium_priority:
            st.warning(f"üìä {rec}")
    
    with col3:
        st.markdown("#### üìà Strat√©gique (3-12 mois)")
        
        for rec in long_term:
            st.info(f"üéØ {rec}")
    
    # ROI estim√© des recommandations
    st.markdown("#### üí∞ Impact Financier Estim√©")
    
    roi_estimates = calculate_recommendation_roi(csv_data, high_priority + medium_priority)
    
    if roi_estimates:
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric("ROI Potentiel", f"{roi_estimates['total_roi']:.1f}%")
        
        with col2:
            st.metric("Impact Revenue", f"+{roi_estimates['revenue_impact']:,.0f} DHS")
        
        with col3:
            st.metric("√âconomies Costs", f"-{roi_estimates['cost_savings']:,.0f} DHS")
    
    # Plan d'action d√©taill√©
    st.markdown("#### üìã Plan d'Action D√©taill√©")
    
    action_plan = create_detailed_action_plan(csv_data)
    
    for i, action in enumerate(action_plan, 1):
        with st.expander(f"Action {i}: {action['title']}"):
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**D√©tails:**")
                st.write(action['description'])
                
                st.markdown("**KPIs √† Suivre:**")
                for kpi in action['kpis']:
                    st.write(f"‚Ä¢ {kpi}")
            
            with col2:
                st.metric("Priorit√©", action['priority'])
                st.metric("Dur√©e Estim√©e", action['duration'])
                st.metric("ROI Attendu", f"{action['expected_roi']}%")
                
                if action['difficulty'] == 'Facile':
                    st.success("‚úÖ Facile √† impl√©menter")
                elif action['difficulty'] == 'Moyen':
                    st.warning("‚ö†Ô∏è Complexit√© mod√©r√©e")
                else:
                    st.error("üî¥ Haute complexit√©")

def generate_strategic_recommendations(csv_data):
    """G√©n√®re des recommandations strat√©giques par priorit√©"""
    
    high_priority = []
    medium_priority = []
    long_term = []
    
    profit_margin = csv_data.get('profit_margin', 0)
    revenue_growth = csv_data.get('revenue_growth', 0)
    current_ratio = csv_data.get('current_ratio', 1.5)
    total_locations = csv_data.get('total_locations', 0)
    ltv_cac = csv_data.get('ltv_cac_ratio', 0)
    
    # Priorit√© haute - Actions urgentes
    if profit_margin < 5:
        high_priority.append("Audit complet des co√ªts et r√©duction imm√©diate de 20%")
    
    if current_ratio < 1.1:
        high_priority.append("Am√©lioration urgente de la tr√©sorerie - n√©gociation fournisseurs")
    
    if revenue_growth < -10:
        high_priority.append("Plan de relance commercial - task force revenue")
    
    # Priorit√© moyenne - Optimisations importantes
    if profit_margin < 12:
        medium_priority.append("Optimisation de la structure des co√ªts variables")
    
    if revenue_growth < 5:
        medium_priority.append("Strat√©gie d'acc√©l√©ration de croissance - nouveaux march√©s")
    
    if total_locations > 1:
        clusters = csv_data.get('geographic_clusters', [])
        low_perf_clusters = [c for c in clusters if c.get('performance_level') in ['Low', 'Poor']]
        if low_perf_clusters:
            medium_priority.append(f"Optimisation de {len(low_perf_clusters)} zone(s) sous-performante(s)")
    
    if 0 < ltv_cac < 2.5:
        medium_priority.append("Am√©lioration du ratio LTV/CAC - optimisation acquisition")
    
    # Strat√©gique long terme
    if total_locations > 0:
        long_term.append("D√©veloppement d'une strat√©gie d'expansion g√©ographique data-driven")
    
    if profit_margin > 15:
        long_term.append("Exploration d'opportunit√©s d'acquisition ou diversification")
    
    long_term.append("Mise en place d'un syst√®me de BI avanc√© pour pilotage temps r√©el")
    
    if revenue_growth > 20:
        long_term.append("Pr√©paration √† la scalabilit√© - infrastructure et processus")
    
    return high_priority, medium_priority, long_term

def calculate_recommendation_roi(csv_data, recommendations):
    """Calcule le ROI estim√© des recommandations"""
    
    current_revenue = csv_data.get('revenue', csv_data.get('monthly_revenue', 0) * 12)
    current_costs = csv_data.get('total_costs', current_revenue * 0.75)
    
    if current_revenue == 0:
        return None
    
    # Estimations bas√©es sur les types de recommandations
    revenue_impact = 0
    cost_savings = 0
    
    for rec in recommendations:
        if 'co√ªts' in rec.lower() and '20%' in rec:
            cost_savings += current_costs * 0.15  # 15% √©conomies r√©alistes
        elif 'croissance' in rec.lower() or 'revenue' in rec.lower():
            revenue_impact += current_revenue * 0.08  # 8% croissance
        elif 'optimisation' in rec.lower():
            cost_savings += current_costs * 0.05  # 5% √©conomies
            revenue_impact += current_revenue * 0.03  # 3% revenue
        elif 'zone' in rec.lower() or 'g√©ographique' in rec.lower():
            revenue_impact += current_revenue * 0.06  # 6% am√©lioration zones
    
    total_impact = revenue_impact + cost_savings
    investment_needed = current_revenue * 0.02  # 2% investment estim√©
    
    total_roi = ((total_impact - investment_needed) / investment_needed * 100) if investment_needed > 0 else 0
    
    return {
        'total_roi': total_roi,
        'revenue_impact': revenue_impact,
        'cost_savings': cost_savings
    }

def create_detailed_action_plan(csv_data):
    """Cr√©e un plan d'action d√©taill√©"""
    
    action_plan = []
    
    profit_margin = csv_data.get('profit_margin', 0)
    revenue_growth = csv_data.get('revenue_growth', 0)
    total_locations = csv_data.get('total_locations', 0)
    
    # Action 1: Optimisation financi√®re
    if profit_margin < 15:
        action_plan.append({
            'title': 'Optimisation de la Rentabilit√©',
            'description': 'Audit complet des co√ªts, ren√©gociation fournisseurs, et optimisation des processus op√©rationnels pour am√©liorer les marges.',
            'priority': 'Critique' if profit_margin < 5 else 'Haute',
            'duration': '2-4 semaines',
            'expected_roi': 25,
            'difficulty': 'Moyen',
            'kpis': [
                'Marge brute +3-5%',
                'R√©duction co√ªts variables 10-15%',
                'D√©lai paiement fournisseurs +7 jours'
            ]
        })
    
    # Action 2: Croissance
    if revenue_growth < 10:
        action_plan.append({
            'title': 'Acc√©l√©ration de la Croissance',
            'description': 'D√©veloppement de nouveaux canaux d\'acquisition, optimisation du marketing digital, et expansion sur de nouveaux segments.',
            'priority': 'Haute',
            'duration': '6-12 semaines',
            'expected_roi': 35,
            'difficulty': 'Moyen',
            'kpis': [
                'Croissance revenue +15%',
                'Nouveaux clients +25%',
                'CAC optimis√© -20%'
            ]
        })
    
    # Action 3: G√©ographique
    if total_locations > 1:
        action_plan.append({
            'title': 'Optimisation G√©ographique',
            'description': 'Analyse approfondie des performances par zone, r√©plication des best practices, et optimisation de la couverture territoriale.',
            'priority': 'Moyenne',
            'duration': '4-8 semaines',
            'expected_roi': 20,
            'difficulty': 'Facile',
            'kpis': [
                'Performance zones faibles +30%',
                'Homog√©n√©isation des marges',
                'Optimisation logistique -15%'
            ]
        })
    
    # Action 4: Digitalisation
    action_plan.append({
        'title': 'Transformation Digitale',
        'description': 'Mise en place d\'outils de BI avanc√©s, automatisation des processus, et d√©veloppement d\'une culture data-driven.',
        'priority': 'Strat√©gique',
        'duration': '12-24 semaines',
        'expected_roi': 45,
        'difficulty': '√âlev√©',
        'kpis': [
            'Temps de reporting -70%',
            'Pr√©cision pr√©visions +40%',
            'Productivit√© √©quipe +25%'
        ]
    })
    
    return action_plan

# ========== ENHANCED SCENARIO PLANNING ==========
def show_enhanced_scenario_planning():
    """Planification de sc√©narios enrichie avec g√©olocalisation"""
    st.header("üéØ Planification de Sc√©narios Avanc√©e")
    
    # R√©cup√©ration des donn√©es enrichies
    csv_data = EnhancedCSVDataManager.get_csv_financial_data()
    
    if not csv_data:
        st.warning("üì§ **Donn√©es CSV non disponibles**")
        st.info("La planification de sc√©narios n√©cessite vos donn√©es CSV pour des projections pr√©cises.")
        
        if st.button("üì§ Importer Donn√©es CSV", type="primary"):
            st.session_state['current_page'] = 'csv_import'
            st.rerun()
        return
    
    st.success("üìä **Sc√©narios bas√©s sur vos donn√©es enrichies**")
    
    # Donn√©es de base enrichies
    base_monthly_revenue = csv_data.get('monthly_revenue', 15000)
    base_monthly_costs = csv_data.get('monthly_costs', 12000)
    current_growth_rate = csv_data.get('revenue_growth', 0) / 100
    profit_margin = csv_data.get('profit_margin', 0)
    total_locations = csv_data.get('total_locations', 0)
    
    st.subheader(f"üìä Donn√©es de Base (issues de votre CSV)")
    
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Revenue Mensuel", f"{base_monthly_revenue:,.0f} DHS")
    with col2:
        st.metric("Co√ªts Mensuels", f"{base_monthly_costs:,.0f} DHS")
    with col3:
        st.metric("Croissance Actuelle", f"{current_growth_rate*100:+.1f}%")
    with col4:
        if total_locations > 0:
            st.metric("Emplacements", total_locations)
        else:
            st.metric("Marge Profit", f"{profit_margin:.1f}%")
    
    # Configuration des sc√©narios enrichie
    st.subheader("‚öôÔ∏è Configuration des Sc√©narios Enrichis")
    
    # Onglets pour diff√©rents types de sc√©narios
    scenario_tabs = st.tabs([
        "üìà Sc√©narios Classiques", 
        "üåç Sc√©narios G√©ographiques", 
        "üöÄ Sc√©narios d'Expansion", 
        "‚ö° Sc√©narios de Crise"
    ])
    
    with scenario_tabs[0]:
        # Sc√©narios financiers classiques
        st.markdown("#### üíº Sc√©narios Financiers Standards")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown("### üò∞ Pessimiste")
            pess_revenue = st.slider("Variation Revenue (%)", -50, 10, max(-25, int(current_growth_rate*100-20)), key="pess_rev")
            pess_cost = st.slider("Variation Co√ªts (%)", -10, 50, 20, key="pess_cost")
            pess_prob = st.slider("Probabilit√© (%)", 5, 40, 25, key="pess_prob")
        
        with col2:
            st.markdown("### üòê R√©aliste")
            real_revenue = st.slider("Variation Revenue (%)", -10, 40, max(5, int(current_growth_rate*100)), key="real_rev")
            real_cost = st.slider("Variation Co√ªts (%)", 0, 30, 10, key="real_cost")
            real_prob = st.slider("Probabilit√© (%)", 40, 80, 55, key="real_prob")
        
        with col3:
            st.markdown("### üòÑ Optimiste")
            opt_revenue = st.slider("Variation Revenue (%)", 10, 80, max(30, int(current_growth_rate*100+25)), key="opt_rev")
            opt_cost = st.slider("Variation Co√ªts (%)", -10, 20, 5, key="opt_cost")
            opt_prob = st.slider("Probabilit√© (%)", 5, 40, 20, key="opt_prob")
    
    with scenario_tabs[1]:
        # Sc√©narios g√©ographiques
        st.markdown("#### üåç Sc√©narios d'Expansion G√©ographique")
        
        if total_locations > 0:
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("### üìç Expansion Locale")
                local_new_sites = st.number_input("Nouveaux Sites Locaux", min_value=0, max_value=10, value=2)
                local_revenue_per_site = st.number_input("Revenue/Site (DHS)", min_value=5000, max_value=50000, value=int(base_monthly_revenue * 0.7))
                local_setup_cost = st.number_input("Co√ªt Installation/Site", min_value=10000, max_value=200000, value=50000)
            
            with col2:
                st.markdown("### üöÄ Expansion R√©gionale")
                regional_new_sites = st.number_input("Nouveaux Sites R√©gionaux", min_value=0, max_value=20, value=5)
                regional_revenue_per_site = st.number_input("Revenue/Site R√©gional", min_value=3000, max_value=40000, value=int(base_monthly_revenue * 0.5))
                regional_setup_cost = st.number_input("Co√ªt Installation R√©gional", min_value=20000, max_value=300000, value=75000)
        else:
            st.info("üí° Sc√©narios g√©ographiques disponibles avec des donn√©es multi-sites")
            
            # Sc√©nario d'expansion depuis un site unique
            st.markdown("### üåç Premi√®re Expansion")
            first_expansion_sites = st.number_input("Nouveaux Emplacements", min_value=1, max_value=5, value=2)
            expansion_revenue_ratio = st.slider("% Revenue vs Site Principal", 30, 100, 70, key="expansion_ratio")
            expansion_timeline = st.selectbox("D√©lai d'Expansion", ["3 mois", "6 mois", "12 mois"])
    
    with scenario_tabs[2]:
        # Sc√©narios d'expansion business
        st.markdown("#### üöÄ Sc√©narios d'Expansion Business")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("### üí∞ Acquisition")
            acquisition_cost = st.number_input("Co√ªt Acquisition (DHS)", min_value=100000, max_value=10000000, value=500000)
            acquisition_revenue_boost = st.slider("Boost Revenue (%)", 20, 200, 50, key="acq_boost")
            acquisition_synergies = st.slider("Synergies Co√ªts (%)", 5, 30, 15, key="acq_synergies")
        
        with col2:
            st.markdown("### üî¨ Innovation")
            innovation_investment = st.number_input("Investment R&D", min_value=50000, max_value=2000000, value=200000)
            innovation_revenue_impact = st.slider("Impact Revenue (%)", 10, 100, 25, key="innov_impact")
            innovation_timeline = st.selectbox("D√©lai ROI Innovation", ["6 mois", "12 mois", "18 mois", "24 mois"])
    
    with scenario_tabs[3]:
        # Sc√©narios de crise
        st.markdown("#### ‚ö° Sc√©narios de Gestion de Crise")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.markdown("### ü¶† Crise Sanitaire")
            health_crisis_revenue_drop = st.slider("Baisse Revenue (%)", 10, 70, 30, key="health_drop")
            health_crisis_duration = st.selectbox("Dur√©e Crise", ["3 mois", "6 mois", "12 mois"], key="health_duration")
            remote_work_savings = st.slider("√âconomies T√©l√©travail (%)", 5, 25, 10, key="remote_savings")
        
        with col2:
            st.markdown("### üí∏ Crise √âconomique")
            economic_crisis_impact = st.slider("Impact √âconomique (%)", 15, 60, 25, key="economic_impact")
            customer_payment_delays = st.slider("Retards Paiements (jours)", 15, 90, 30, key="payment_delays")
            cost_reduction_potential = st.slider("R√©duction Co√ªts (%)", 10, 40, 20, key="cost_reduction")
        
        with col3:
            st.markdown("### üî• Crise Op√©rationnelle")
            operational_crisis_sites = st.number_input("Sites Impact√©s", min_value=1, max_value=total_locations if total_locations > 0 else 1, value=1)
            recovery_timeline = st.selectbox("D√©lai R√©cup√©ration", ["1 mois", "3 mois", "6 mois"], key="recovery_time")
            insurance_coverage = st.slider("Couverture Assurance (%)", 0, 100, 70, key="insurance")
    
    # Param√®tres globaux
    col1, col2 = st.columns(2)
    
    with col1:
        analysis_period = st.selectbox("P√©riode d'Analyse", [12, 18, 24, 36], index=1)
        include_seasonality = st.checkbox("Inclure Saisonnalit√©", value=True)
    
    with col2:
        monte_carlo_simulations = st.selectbox("Simulations Monte Carlo", [100, 500, 1000, 2000], index=1)
        confidence_interval = st.selectbox("Intervalle de Confiance", ["80%", "90%", "95%", "99%"], index=2)
    
    # Ex√©cution de l'analyse de sc√©narios
    if st.button("üöÄ Lancer l'Analyse de Sc√©narios Avanc√©e", type="primary"):
        with st.spinner("Ex√©cution des simulations avanc√©es..."):
            
            # Validation des probabilit√©s
            total_prob = pess_prob + real_prob + opt_prob
            if total_prob != 100:
                st.warning(f"‚ö†Ô∏è Ajustement des probabilit√©s (total: {total_prob}%)")
                pess_prob = pess_prob * 100 / total_prob
                real_prob = real_prob * 100 / total_prob
                opt_prob = opt_prob * 100 / total_prob
            
            # Construction des sc√©narios enrichis
            scenarios = build_enhanced_scenarios(
                base_monthly_revenue, base_monthly_costs, analysis_period,
                pess_revenue, pess_cost, pess_prob,
                real_revenue, real_cost, real_prob,
                opt_revenue, opt_cost, opt_prob,
                csv_data, include_seasonality
            )
            
            # Ex√©cution Monte Carlo
            mc_results = run_monte_carlo_analysis(
                scenarios, monte_carlo_simulations, analysis_period, csv_data
            )
            
            # Stockage des r√©sultats
            st.session_state.enhanced_scenario_results = {
                'scenarios': scenarios,
                'monte_carlo': mc_results,
                'parameters': {
                    'analysis_period': analysis_period,
                    'confidence_interval': confidence_interval,
                    'simulations': monte_carlo_simulations
                }
            }
    
    # Affichage des r√©sultats enrichis
    if 'enhanced_scenario_results' in st.session_state:
        display_enhanced_scenario_results(st.session_state.enhanced_scenario_results, csv_data)

def build_enhanced_scenarios(base_revenue, base_costs, periods, 
                           pess_rev, pess_cost, pess_prob,
                           real_rev, real_cost, real_prob,
                           opt_rev, opt_cost, opt_prob,
                           csv_data, include_seasonality):
    """Construit des sc√©narios enrichis avec saisonnalit√©"""
    
    scenarios = {}
    
    # Facteurs saisonniers (bas√©s sur les donn√©es ou d√©faut retail)
    seasonal_factors = [0.85, 0.9, 1.1, 1.05, 1.0, 0.95, 0.9, 0.95, 1.0, 1.1, 1.2, 1.3] if include_seasonality else [1.0] * 12
    
    # Construction de chaque sc√©nario
    scenario_configs = [
        ('pessimistic', pess_rev, pess_cost, pess_prob),
        ('realistic', real_rev, real_cost, real_prob),
        ('optimistic', opt_rev, opt_cost, opt_prob)
    ]
    
    for scenario_name, rev_change, cost_change, probability in scenario_configs:
        monthly_results = []
        
        for month in range(periods):
            # Facteur saisonnier
            seasonal_factor = seasonal_factors[month % 12]
            
            # √âvolution progressive sur la p√©riode
            progress_factor = 1 + (month / periods) * 0.1  # √âvolution progressive
            
            # Calcul du mois
            monthly_revenue = base_revenue * (1 + rev_change/100) * seasonal_factor * progress_factor
            monthly_cost = base_costs * (1 + cost_change/100) * progress_factor
            monthly_profit = monthly_revenue - monthly_cost
            
            # M√©triques enrichies
            monthly_margin = (monthly_profit / monthly_revenue * 100) if monthly_revenue > 0 else 0
            cumulative_profit = sum([r['profit'] for r in monthly_results]) + monthly_profit
            
            monthly_results.append({
                'month': month + 1,
                'revenue': monthly_revenue,
                'cost': monthly_cost,
                'profit': monthly_profit,
                'margin': monthly_margin,
                'cumulative_profit': cumulative_profit,
                'seasonal_factor': seasonal_factor
            })
        
        # M√©triques globales du sc√©nario
        total_revenue = sum(m['revenue'] for m in monthly_results)
        total_cost = sum(m['cost'] for m in monthly_results)
        total_profit = sum(m['profit'] for m in monthly_results)
        avg_margin = np.mean([m['margin'] for m in monthly_results])
        
        scenarios[scenario_name] = {
            'monthly_data': monthly_results,
            'total_revenue': total_revenue,
            'total_cost': total_cost,
            'total_profit': total_profit,
            'avg_margin': avg_margin,
            'probability': probability / 100,
            'roi': (total_profit / total_cost * 100) if total_cost > 0 else 0
        }
    
    return scenarios

def run_monte_carlo_analysis(scenarios, num_simulations, periods, csv_data):
    """Ex√©cute une analyse Monte Carlo avanc√©e"""
    
    # Param√®tres de volatilit√© bas√©s sur les donn√©es CSV
    revenue_volatility = csv_data.get('revenue_volatility', 0.15)
    cost_volatility = revenue_volatility * 0.7  # Co√ªts g√©n√©ralement moins volatils
    
    mc_results = []
    
    base_revenue = csv_data.get('monthly_revenue', 15000)
    base_costs = csv_data.get('monthly_costs', 12000)
    
    for sim in range(num_simulations):
        # S√©lection al√©atoire du sc√©nario bas√© sur les probabilit√©s
        rand = np.random.random()
        cumulative_prob = 0
        selected_scenario = 'realistic'
        
        for scenario_name, scenario_data in scenarios.items():
            cumulative_prob += scenario_data['probability']
            if rand <= cumulative_prob:
                selected_scenario = scenario_name
                break
        
        # Simulation avec volatilit√©
        sim_revenue_path = []
        sim_cost_path = []
        sim_profit_path = []
        
        scenario = scenarios[selected_scenario]
        
        for month in range(periods):
            base_month_revenue = scenario['monthly_data'][month]['revenue']
            base_month_cost = scenario['monthly_data'][month]['cost']
            
            # Ajout de bruit stochastique
            revenue_shock = np.random.normal(1, revenue_volatility)
            cost_shock = np.random.normal(1, cost_volatility)
            
            sim_revenue = base_month_revenue * revenue_shock
            sim_cost = base_month_cost * cost_shock
            sim_profit = sim_revenue - sim_cost
            
            sim_revenue_path.append(sim_revenue)
            sim_cost_path.append(sim_cost)
            sim_profit_path.append(sim_profit)
        
        # M√©triques de la simulation
        total_sim_revenue = sum(sim_revenue_path)
        total_sim_cost = sum(sim_cost_path)
        total_sim_profit = sum(sim_profit_path)
        
        # Calcul du maximum drawdown
        cumulative_profits = np.cumsum(sim_profit_path)
        running_max = np.maximum.accumulate(cumulative_profits)
        drawdowns = (cumulative_profits - running_max)
        max_drawdown = drawdowns.min()
        
        mc_results.append({
            'scenario': selected_scenario,
            'total_revenue': total_sim_revenue,
            'total_cost': total_sim_cost,
            'total_profit': total_sim_profit,
            'final_margin': (total_sim_profit / total_sim_revenue * 100) if total_sim_revenue > 0 else 0,
            'max_drawdown': max_drawdown,
            'profit_volatility': np.std(sim_profit_path),
            'break_even_month': next((i for i, cp in enumerate(np.cumsum(sim_profit_path)) if cp > 0), periods),
            'revenue_path': sim_revenue_path,
            'cost_path': sim_cost_path,
            'profit_path': sim_profit_path
        })
    
    return pd.DataFrame(mc_results)

def display_enhanced_scenario_results(results, csv_data):
    """Affiche les r√©sultats enrichis de l'analyse de sc√©narios"""
    
    scenarios = results['scenarios']
    mc_results = results['monte_carlo']
    params = results['parameters']
    
    st.subheader("üìä R√©sultats de l'Analyse de Sc√©narios Enrichie")
    
    # M√©triques de synth√®se
    col1, col2, col3, col4, col5 = st.columns(5)
    
    expected_profit = sum(data['total_profit'] * data['probability'] for data in scenarios.values())
    best_case = max(data['total_profit'] for data in scenarios.values())
    worst_case = min(data['total_profit'] for data in scenarios.values())
    
    # M√©triques Monte Carlo
    mc_mean_profit = mc_results['total_profit'].mean()
    mc_std_profit = mc_results['total_profit'].std()
    
    with col1:
        st.metric("üí∞ Profit Attendu", f"{expected_profit:,.0f} DHS")
        st.metric("üìä Monte Carlo Moyen", f"{mc_mean_profit:,.0f} DHS")
    
    with col2:
        st.metric("üöÄ Meilleur Cas", f"{best_case:,.0f} DHS", f"+{best_case - expected_profit:,.0f}")
    
    with col3:
        st.metric("‚ö†Ô∏è Pire Cas", f"{worst_case:,.0f} DHS", f"{worst_case - expected_profit:,.0f}")
    
    with col4:
        profit_range = best_case - worst_case
        st.metric("üìè Fourchette", f"{profit_range:,.0f} DHS")
        
        # Coefficient de variation
        cv = (mc_std_profit / mc_mean_profit * 100) if mc_mean_profit != 0 else 0
        st.metric("üìä Volatilit√©", f"{cv:.1f}%")
    
    with col5:
        # Probabilit√© de succ√®s (profit > 0)
        success_prob = (mc_results['total_profit'] > 0).sum() / len(mc_results) * 100
        st.metric("üéØ Prob. Succ√®s", f"{success_prob:.1f}%")
        
        # VaR (Value at Risk) √† 95%
        var_95 = np.percentile(mc_results['total_profit'], 5)
        st.metric("‚ö†Ô∏è VaR 95%", f"{var_95:,.0f} DHS")
    
    # Visualisations enrichies
    st.subheader("üìà Visualisations Avanc√©es")
    
    viz_tabs = st.tabs([
        "üìä Comparaison Sc√©narios", 
        "üé≤ Analyse Monte Carlo", 
        "üìà √âvolution Temporelle",
        "‚ö†Ô∏è Analyse des Risques"
    ])
    
    with viz_tabs[0]:
        # Comparaison des sc√©narios
        fig_scenarios = create_scenario_comparison_chart(scenarios)
        st.plotly_chart(fig_scenarios, use_container_width=True)
        
        # Tableau de comparaison
        st.markdown("#### üìã Tableau de Comparaison D√©taill√©")
        
        comparison_data = []
        for scenario, data in scenarios.items():
            comparison_data.append({
                'Sc√©nario': scenario.title(),
                'Profit Total': f"{data['total_profit']:,.0f} DHS",
                'Marge Moyenne': f"{data['avg_margin']:.1f}%",
                'ROI': f"{data['roi']:.1f}%",
                'Probabilit√©': f"{data['probability']:.0%}",
                'Contribution Attendue': f"{data['total_profit'] * data['probability']:,.0f} DHS"
            })
        
        st.dataframe(pd.DataFrame(comparison_data), use_container_width=True)
    
    with viz_tabs[1]:
        # Analyse Monte Carlo
        fig_mc = create_monte_carlo_distribution_chart(mc_results)
        st.plotly_chart(fig_mc, use_container_width=True)
        
        # Statistiques Monte Carlo d√©taill√©es
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("#### üìä Statistiques Descriptives")
            
            mc_stats = {
                'Moyenne': mc_results['total_profit'].mean(),
                'M√©diane': mc_results['total_profit'].median(),
                '√âcart-type': mc_results['total_profit'].std(),
                'Minimum': mc_results['total_profit'].min(),
                'Maximum': mc_results['total_profit'].max()
            }
            
            for stat, value in mc_stats.items():
                st.metric(stat, f"{value:,.0f} DHS")
        
        with col2:
            st.markdown("#### üéØ Percentiles de Performance")
            
            percentiles = [5, 25, 50, 75, 95]
            perc_values = np.percentile(mc_results['total_profit'], percentiles)
            
            for p, v in zip(percentiles, perc_values):
                st.metric(f"{p}e percentile", f"{v:,.0f} DHS")
    
    with viz_tabs[2]:
        # √âvolution temporelle
        fig_temporal = create_temporal_evolution_chart(scenarios, mc_results)
        st.plotly_chart(fig_temporal, use_container_width=True)
        
        # Analyse de la saisonnalit√©
        if any('seasonal_factor' in month for scenario in scenarios.values() for month in scenario['monthly_data']):
            st.markdown("#### üåä Impact de la Saisonnalit√©")
            
            seasonal_impact = analyze_seasonal_impact(scenarios)
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.metric("Mois le Plus Fort", seasonal_impact['best_month'])
                st.metric("Performance Max", f"+{seasonal_impact['max_boost']:.1f}%")
            
            with col2:
                st.metric("Mois le Plus Faible", seasonal_impact['worst_month'])
                st.metric("Impact Min", f"{seasonal_impact['min_impact']:.1f}%")
    
    with viz_tabs[3]:
        # Analyse des risques
        fig_risk = create_risk_analysis_chart(mc_results)
        st.plotly_chart(fig_risk, use_container_width=True)
        
        # M√©triques de risque avanc√©es
        st.markdown("#### ‚ö†Ô∏è M√©triques de Risque Avanc√©es")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            # Expected Shortfall (CVaR)
            var_95 = np.percentile(mc_results['total_profit'], 5)
            cvar_95 = mc_results[mc_results['total_profit'] <= var_95]['total_profit'].mean()
            
            st.metric("VaR 95%", f"{var_95:,.0f} DHS")
            st.metric("CVaR 95%", f"{cvar_95:,.0f} DHS")
        
        with col2:
            # Drawdown analysis
            avg_max_drawdown = mc_results['max_drawdown'].mean()
            worst_drawdown = mc_results['max_drawdown'].min()
            
            st.metric("Drawdown Moyen", f"{avg_max_drawdown:,.0f} DHS")
            st.metric("Pire Drawdown", f"{worst_drawdown:,.0f} DHS")
        
        with col3:
            # Break-even analysis
            avg_break_even = mc_results['break_even_month'].mean()
            break_even_prob = (mc_results['break_even_month'] <= params['analysis_period']).sum() / len(mc_results) * 100
            
            st.metric("Break-even Moyen", f"{avg_break_even:.1f} mois")
            st.metric("Prob. Break-even", f"{break_even_prob:.1f}%")
    
    # Recommandations strat√©giques
    st.subheader("üí° Recommandations Strat√©giques")
    
    strategic_recs = generate_scenario_recommendations(scenarios, mc_results, csv_data)
    
    for i, rec in enumerate(strategic_recs, 1):
        st.warning(f"**Recommandation {i}**: {rec}")
    
    # Export des r√©sultats
    st.subheader("üì§ Export des R√©sultats")
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        if st.button("üíæ Sauvegarder Analyse", type="primary"):
            st.success("‚úÖ Analyse de sc√©narios sauvegard√©e!")
    
    with col2:
        # Export CSV des r√©sultats Monte Carlo
        if st.button("üìä Exporter Monte Carlo"):
            csv_export = mc_results.to_csv(index=False)
            st.download_button(
                label="üíæ T√©l√©charger R√©sultats CSV",
                data=csv_export,
                file_name=f"monte_carlo_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                mime="text/csv"
            )
    
    with col3:
        if st.button("üìà Vers Forecasting ML"):
            st.session_state['current_page'] = 'ml_forecasting'
            st.rerun()

def create_scenario_comparison_chart(scenarios):
    """Cr√©e un graphique de comparaison des sc√©narios"""
    
    scenario_names = list(scenarios.keys())
    profits = [scenarios[s]['total_profit'] for s in scenario_names]
    margins = [scenarios[s]['avg_margin'] for s in scenario_names]
    probabilities = [scenarios[s]['probability'] * 100 for s in scenario_names]
    
    # Graphique en barres avec double axe Y
    fig = make_subplots(
        rows=1, cols=2,
        subplot_titles=('Profit Total par Sc√©nario', 'Marges Moyennes'),
        specs=[[{"secondary_y": False}, {"secondary_y": True}]]
    )
    
    # Barres de profit
    colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']
    
    fig.add_trace(
        go.Bar(
            x=[s.title() for s in scenario_names],
            y=profits,
            name='Profit Total',
            marker_color=colors,
            text=[f"{p:,.0f} DHS" for p in profits],
            textposition='auto'
        ),
        row=1, col=1
    )
    
    # Marges moyennes
    fig.add_trace(
        go.Bar(
            x=[s.title() for s in scenario_names],
            y=margins,
            name='Marge Moyenne (%)',
            marker_color=[f"rgba{tuple(list(__import__('matplotlib.colors').to_rgb(c)) + [0.7])}" for c in colors],
            text=[f"{m:.1f}%" for m in margins],
            textposition='auto'
        ),
        row=1, col=2
    )
    
    fig.update_layout(
        height=400,
        title_text="Comparaison des Sc√©narios Financiers",
        showlegend=False
    )
    
    return fig

def create_monte_carlo_distribution_chart(mc_results):
    """Cr√©e un graphique de distribution Monte Carlo"""
    
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=('Distribution des Profits', 'Profits vs Marges', '√âvolution des Sc√©narios', 'Box Plot par Sc√©nario'),
        specs=[[{"type": "histogram"}, {"type": "scatter"}],
               [{"type": "scatter"}, {"type": "box"}]]
    )
    
    # Histogramme des profits
    fig.add_trace(
        go.Histogram(
            x=mc_results['total_profit'],
            nbinsx=50,
            name='Distribution Profits',
            marker_color='lightblue',
            opacity=0.7
        ),
        row=1, col=1
    )
    
    # Scatter profits vs marges
    fig.add_trace(
        go.Scatter(
            x=mc_results['total_profit'],
            y=mc_results['final_margin'],
            mode='markers',
            name='Profit vs Marge',
            marker=dict(
                color=mc_results['profit_volatility'],
                colorscale='Viridis',
                showscale=True,
                size=6,
                opacity=0.6
            )
        ),
        row=1, col=2
    )
    
    # √âvolution par sc√©nario
    scenario_colors = {'pessimistic': '#FF6B6B', 'realistic': '#4ECDC4', 'optimistic': '#45B7D1'}
    
    for scenario in mc_results['scenario'].unique():
        scenario_data = mc_results[mc_results['scenario'] == scenario]
        
        fig.add_trace(
            go.Scatter(
                x=list(range(len(scenario_data))),
                y=scenario_data['total_profit'],
                mode='markers',
                name=scenario.title(),
                marker=dict(color=scenario_colors.get(scenario, 'gray'), size=4, opacity=0.6)
            ),
            row=2, col=1
        )
    
    # Box plot par sc√©nario
    for scenario in mc_results['scenario'].unique():
        scenario_data = mc_results[mc_results['scenario'] == scenario]
        
        fig.add_trace(
            go.Box(
                y=scenario_data['total_profit'],
                name=scenario.title(),
                marker_color=scenario_colors.get(scenario, 'gray')
            ),
            row=2, col=2
        )
    
    fig.update_layout(
        height=600,
        title_text="Analyse Monte Carlo Compl√®te",
        showlegend=False
    )
    
    return fig

def create_temporal_evolution_chart(scenarios, mc_results):
    """Cr√©e un graphique d'√©volution temporelle"""
    
    fig = go.Figure()
    
    colors = {'pessimistic': '#FF6B6B', 'realistic': '#4ECDC4', 'optimistic': '#45B7D1'}
    
    # Lignes des sc√©narios d√©terministes
    for scenario_name, scenario_data in scenarios.items():
        months = [m['month'] for m in scenario_data['monthly_data']]
        cumulative_profits = [m['cumulative_profit'] for m in scenario_data['monthly_data']]
        
        fig.add_trace(go.Scatter(
            x=months,
            y=cumulative_profits,
            mode='lines+markers',
            name=f"{scenario_name.title()} (D√©terministe)",
            line=dict(color=colors[scenario_name], width=3),
            marker=dict(size=8)
        ))
    
    # Bandes de confiance Monte Carlo
    if len(mc_results) > 0:
        # Calculer les percentiles pour chaque mois
        max_months = max(len(path) for path in mc_results['profit_path'])
        
        percentiles_5 = []
        percentiles_95 = []
        medians = []
        
        for month in range(max_months):
            month_profits = []
            for idx, profit_path in enumerate(mc_results['profit_path']):
                if month < len(profit_path):
                    cumulative_profit = sum(profit_path[:month+1])
                    month_profits.append(cumulative_profit)
            
            if month_profits:
                percentiles_5.append(np.percentile(month_profits, 5))
                percentiles_95.append(np.percentile(month_profits, 95))
                medians.append(np.percentile(month_profits, 50))
        
        months_range = list(range(1, len(percentiles_5) + 1))
        
        # Bande de confiance
        fig.add_trace(go.Scatter(
            x=months_range + months_range[::-1],
            y=percentiles_95 + percentiles_5[::-1],
            fill='toself',
            fillcolor='rgba(128,128,128,0.2)',
            line=dict(color='rgba(255,255,255,0)'),
            name='Intervalle de Confiance 90%',
            showlegend=True
        ))
        
        # M√©diane Monte Carlo
        fig.add_trace(go.Scatter(
            x=months_range,
            y=medians,
            mode='lines',
            name='M√©diane Monte Carlo',
            line=dict(color='gray', width=2, dash='dash')
        ))
    
    fig.add_hline(y=0, line_dash="dash", line_color="black", annotation_text="Break-even")
    
    fig.update_layout(
        title="√âvolution Temporelle - Profit Cumul√©",
        xaxis_title="Mois",
        yaxis_title="Profit Cumul√© (DHS)",
        height=500,
        hovermode='x unified'
    )
    
    return fig

def create_risk_analysis_chart(mc_results):
    """Cr√©e un graphique d'analyse des risques"""
    
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=('Value at Risk (VaR)', 'Drawdown Distribution', 'Volatilit√© des Profits', 'Temps de Break-even'),
        specs=[[{"type": "bar"}, {"type": "histogram"}],
               [{"type": "scatter"}, {"type": "histogram"}]]
    )
    
    # VaR √† diff√©rents niveaux
    var_levels = [1, 5, 10, 25]
    var_values = [np.percentile(mc_results['total_profit'], level) for level in var_levels]
    
    fig.add_trace(
        go.Bar(
            x=[f"VaR {level}%" for level in var_levels],
            y=var_values,
            name='Value at Risk',
            marker_color=['red' if v < 0 else 'orange' if v < np.mean(mc_results['total_profit']) else 'green' for v in var_values],
            text=[f"{v:,.0f}" for v in var_values],
            textposition='auto'
        ),
        row=1, col=1
    )
    
    # Distribution des drawdowns
    fig.add_trace(
        go.Histogram(
            x=mc_results['max_drawdown'],
            nbinsx=30,
            name='Drawdown Distribution',
            marker_color='red',
            opacity=0.7
        ),
        row=1, col=2
    )
    
    # Volatilit√© vs Performance
    fig.add_trace(
        go.Scatter(
            x=mc_results['total_profit'],
            y=mc_results['profit_volatility'],
            mode='markers',
            name='Volatilit√© vs Profit',
            marker=dict(
                color=mc_results['final_margin'],
                colorscale='RdYlGn',
                showscale=True,
                size=6,
                opacity=0.6
            )
        ),
        row=2, col=1
    )
    
    # Distribution temps de break-even
    fig.add_trace(
        go.Histogram(
            x=mc_results['break_even_month'],
            nbinsx=20,
            name='Break-even Timing',
            marker_color='blue',
            opacity=0.7
        ),
        row=2, col=2
    )
    
    fig.update_layout(
        height=600,
        title_text="Analyse Compl√®te des Risques",
        showlegend=False
    )
    
    return fig

def analyze_seasonal_impact(scenarios):
    """Analyse l'impact de la saisonnalit√©"""
    
    # Extraire les facteurs saisonniers du sc√©nario r√©aliste
    realistic_data = scenarios.get('realistic', {}).get('monthly_data', [])
    
    if not realistic_data or 'seasonal_factor' not in realistic_data[0]:
        return {'best_month': 'N/A', 'worst_month': 'N/A', 'max_boost': 0, 'min_impact': 0}
    
    seasonal_factors = [month['seasonal_factor'] for month in realistic_data[:12]]
    month_names = ['Jan', 'F√©v', 'Mar', 'Avr', 'Mai', 'Jun', 
                   'Jul', 'Ao√ª', 'Sep', 'Oct', 'Nov', 'D√©c']
    
    max_idx = np.argmax(seasonal_factors)
    min_idx = np.argmin(seasonal_factors)
    
    return {
        'best_month': month_names[max_idx],
        'worst_month': month_names[min_idx],
        'max_boost': (seasonal_factors[max_idx] - 1) * 100,
        'min_impact': (seasonal_factors[min_idx] - 1) * 100
    }

def generate_scenario_recommendations(scenarios, mc_results, csv_data):
    """G√©n√®re des recommandations bas√©es sur l'analyse de sc√©narios"""
    
    recommendations = []
    
    # Analyse de la distribution des r√©sultats
    success_rate = (mc_results['total_profit'] > 0).sum() / len(mc_results)
    profit_volatility = mc_results['total_profit'].std() / mc_results['total_profit'].mean()
    worst_case_loss = mc_results['total_profit'].min()
    
    # Recommandations bas√©es sur le taux de succ√®s
    if success_rate < 0.7:
        recommendations.append("Taux de succ√®s faible (< 70%) - Revoir la strat√©gie de base ou r√©duire les ambitions")
    elif success_rate > 0.9:
        recommendations.append("Taux de succ√®s √©lev√© - Consid√©rer des objectifs plus ambitieux")
    
    # Recommandations bas√©es sur la volatilit√©
    if profit_volatility > 0.5:
        recommendations.append("Forte volatilit√© d√©tect√©e - Diversifier les sources de revenus et stabiliser les co√ªts")
    
    # Recommandations bas√©es sur les pertes potentielles
    current_equity = csv_data.get('equity', csv_data.get('revenue', 100000) * 0.4)
    if abs(worst_case_loss) > current_equity * 0.3:
        recommendations.append("Risque de perte √©lev√© - Constituer des r√©serves ou r√©duire l'exposition au risque")
    
    # Recommandations sp√©cifiques aux sc√©narios
    realistic_profit = scenarios.get('realistic', {}).get('total_profit', 0)
    optimistic_profit = scenarios.get('optimistic', {}).get('total_profit', 0)
    
    upside_potential = (optimistic_profit - realistic_profit) / realistic_profit if realistic_profit > 0 else 0
    
    if upside_potential > 0.5:
        recommendations.append("Fort potentiel de hausse (+50%) - Pr√©parer les ressources pour capitaliser sur l'optimisme")
    
    # Recommandation g√©ographique si applicable
    if csv_data.get('total_locations', 0) > 1:
        recommendations.append("Optimiser la performance g√©ographique pour r√©duire la variance entre sc√©narios")
    
    return recommendations[:4]  # Limiter √† 4 recommandations principales

# ========== MAIN APPLICATION ==========
def main():
    """Application principale enrichie"""
    
    init_session_state()
    
    # Header enrichi
    st.sidebar.markdown(f"""
    ### üåç AIFI - Suite Financi√®re Avanc√©e
    **Intelligence G√©ographique & Analytique**
    
    *Connect√© en tant que: **{st.session_state.get('user_login', 'SalianiBouchaib')}***
    
    üìÖ **{datetime.now().strftime('%d/%m/%Y %H:%M')}**
    
    ---
    """)
    
    # Indicateur de donn√©es CSV enrichi
    csv_data = EnhancedCSVDataManager.get_csv_financial_data()
    
    if EnhancedCSVDataManager.has_csv_data():
        st.sidebar.success("üìä **Donn√©es CSV Charg√©es**")
        
        # Affichage des m√©triques cl√©s dans la sidebar
        if csv_data:
            st.sidebar.metric("üí∞ Revenue Mensuel", f"{csv_data.get('monthly_revenue', 0):,.0f} DHS")
            st.sidebar.metric("üìä Marge Profit", f"{csv_data.get('profit_margin', 0):.1f}%")
            
            # Indicateur g√©ographique
            total_locations = csv_data.get('total_locations', 0)
            if total_locations > 0:
                st.sidebar.metric("üåç Emplacements", total_locations)
                st.sidebar.metric("üìè √âtendue", f"{csv_data.get('geographic_spread', 0):.0f} km")
            
            # Score de sant√©
            health_score, _ = calculate_enhanced_health_score(csv_data)
            st.sidebar.metric("üéØ Score Sant√©", f"{health_score:.0f}/100")
            
            if health_score >= 75:
                st.sidebar.success("‚úÖ Excellente sant√©")
            elif health_score >= 50:
                st.sidebar.info("üìä Sant√© correcte")
            else:
                st.sidebar.warning("‚ö†Ô∏è Attention requise")
    else:
        st.sidebar.warning("üì§ **Aucune donn√©e CSV**")
        st.sidebar.caption("Importez vos donn√©es pour l'analyse compl√®te")
    
    # Menu de navigation enrichi
    menu_items = {
        "üì§ Import CSV Enrichi": "csv_import",
        "üëî Dashboard Ex√©cutif": "executive_dashboard",
        "üß† Analytics Avanc√©s": "advanced_analytics",
        "üåç Analyse G√©ographique": "geographic_analysis",
        "üéØ Planification Sc√©narios": "scenario_planning", 
        "ü§ñ Forecasting ML": "ml_forecasting",
        "‚ö†Ô∏è Gestion des Risques": "risk_management",
        "üè≠ Templates Sectoriels": "industry_templates"
    }
    
    # Gestion de la navigation
    if 'current_page' in st.session_state:
        page_key = st.session_state['current_page']
        choice = None
        for menu_text, menu_key in menu_items.items():
            if menu_key == page_key:
                choice = menu_text
                break
        if not choice:
            choice = list(menu_items.keys())[0]
        del st.session_state['current_page']
    else:
        choice = st.sidebar.selectbox(
            "üß≠ **Navigation Principale**",
            list(menu_items.keys()),
            index=0
        )
    
    # Routage vers les pages appropri√©es
    page_key = menu_items[choice]
    
    if page_key == "csv_import":
        show_enhanced_csv_import()
    elif page_key == "executive_dashboard":
        show_enhanced_executive_dashboard()
    elif page_key == "advanced_analytics":
        show_enhanced_advanced_analytics()
    elif page_key == "geographic_analysis":
        show_geographic_analysis()
    elif page_key == "scenario_planning":
        show_enhanced_scenario_planning()
    elif page_key == "ml_forecasting":
        show_ml_forecasting()
    elif page_key == "risk_management":
        show_risk_management()
    elif page_key == "industry_templates":
        show_industry_templates()
    
    # Sidebar enrichie avec informations syst√®me
    with st.sidebar:
        st.markdown("---")
        
        # Status syst√®me enrichi
        st.markdown("### üîß **Statut Syst√®me**")
        st.success("üü¢ **Processeur CSV**: Op√©rationnel")
        st.success("üü¢ **Moteur Analytics**: Actif") 
        st.success("üü¢ **IA G√©ographique**: Disponible")
        st.success("üü¢ **ML Forecasting**: Pr√™t")
        st.success("üü¢ **Templates Sectoriels**: Complets")
        
        # Informations de session enrichies
        st.markdown("---")
        st.markdown("### üìä **Session Info**")
        
        session_start = st.session_state.get('session_start_time', datetime.now())
        session_duration = datetime.now() - session_start
        
        st.caption(f"‚è∞ Dur√©e session: {str(session_duration).split('.')[0]}")
        st.caption(f"üïí Heure actuelle: {datetime.now().strftime('%H:%M:%S')}")
        st.caption(f"üë§ Utilisateur: **SalianiBouchaib**")
        st.caption(f"üåç Version: **AIFI v2.0 Enhanced**")
        
        # Fonctionnalit√©s disponibles
        st.markdown("---")
        st.markdown("### ‚ú® **Nouvelles Fonctionnalit√©s**")
        st.caption("‚úÖ **G√©olocalisation Avanc√©e**")
        st.caption("‚úÖ **Intelligence G√©ographique**")
        st.caption("‚úÖ **Analytics Multi-dimensionnels**")
        st.caption("‚úÖ **Clustering Automatique**")
        st.caption("‚úÖ **Pr√©dictions IA Enrichies**")
        st.caption("‚úÖ **Optimisation G√©o-financi√®re**")
        
        # Liens rapides
        st.markdown("---")
        st.markdown("### ‚ö° **Actions Rapides**")
        
        if st.button("üîÑ **Actualiser Donn√©es**", use_container_width=True):
            st.rerun()
        
        if st.button("üíæ **Sauvegarder Session**", use_container_width=True):
            st.success("‚úÖ Session sauvegard√©e!")
        
        if st.button("üì± **Mode Mobile**", use_container_width=True):
            st.info("üì± Interface mobile en d√©veloppement...")

# Point d'entr√©e de l'application
if __name__ == "__main__":
    # Initialiser le temps de session
    if 'session_start_time' not in st.session_state:
        st.session_state.session_start_time = datetime.now()
    
    main()